{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "#\n",
    "# Copyright (C) 2022 Maxim Integrated Products, Inc. All Rights Reserved.\n",
    "#\n",
    "# Maxim Integrated Products, Inc. Default Copyright Notice:\n",
    "# https://www.maximintegrated.com/en/aboutus/legal/copyrights.html\n",
    "#\n",
    "###################################################################################################\n",
    "\n",
    "## IMPORTS\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import distiller\n",
    "\n",
    "## FOR TENSOR BOARD\n",
    "try:\n",
    "    import tensorboard  # pylint: disable=import-error\n",
    "    import tensorflow  # pylint: disable=import-error\n",
    "    tensorflow.io.gfile = tensorboard.compat.tensorflow_stub.io.gfile\n",
    "except (ModuleNotFoundError, AttributeError):\n",
    "    pass\n",
    "\n",
    "import torchnet.meter as tnt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import distiller.apputils as apputils\n",
    "from distiller.data_loggers import PythonLogger, TensorBoardLogger\n",
    "\n",
    "## PATH FOR MODELS AND DATASET\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'models'))\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'datasets'))\n",
    "\n",
    "from datasets import nilm\n",
    "import ai8x\n",
    "mod = importlib.import_module(\"ai87net-unetnilm\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "msglogger = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fn = nilm.ukdale_get_datasets\n",
    "data_path = \"../data/NILM/\"\n",
    "batch_size = 8\n",
    "workers = 5\n",
    "validation_split = 0.1\n",
    "deterministic = True\n",
    "\n",
    "## Variable Declaration for MSG Logger\n",
    "log_prefix = \"ukdale-train\"\n",
    "log_dir = \"jupyter_logging\"\n",
    "dataset_name = \"ukdale\"\n",
    "num_classes = 5\n",
    "model_name = \"cnn1dnilm\"\n",
    "seq_len = 100\n",
    "lr = 1e-4\n",
    "\n",
    "## Variable Declaration for Model\n",
    "quantiles = [0.0025,0.1, 0.5, 0.9, 0.975]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "        def __init__(self, act_mode_8bit):\n",
    "                self.act_mode_8bit = act_mode_8bit\n",
    "                self.truncate_testset = False\n",
    "\n",
    "def count_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params\n",
    "\n",
    "class NormDen:\n",
    "        def __init__(self, mini, maxi):\n",
    "               self.mini = mini\n",
    "               self.maxi = maxi\n",
    "\n",
    "        def normalize(self, data):\n",
    "                data = (data - self.mini) / (self.maxi - self.mini)\n",
    "                return data.sub(0.5).mul(256.).round().clamp(min=-128, max=127).div(128.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSG Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log file for this run: C:\\Dev\\iso\\ai8x-training\\notebooks\\jupyter_logging\\ukdale-train___2024.02.21-120305\\ukdale-train___2024.02.21-120305.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------\n",
      "Logging to TensorBoard - remember to execute the server:\n",
      "> tensorboard --logdir='./logs'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset_name:ukdale\n",
      "dataset_fn=<function ukdale_get_datasets at 0x0000014BB7023430>\n",
      "num_classes=5\n",
      "model_name=cnn1dnilm\\seq_len=100\n",
      "batch_size=8\n",
      "validation_split=0.1\n",
      "lr=0.000100\n"
     ]
    }
   ],
   "source": [
    "msglogger = apputils.config_pylogger('logging.conf', log_prefix,\n",
    "                                        log_dir)\n",
    "\n",
    "pylogger = PythonLogger(msglogger, log_1d=True)\n",
    "all_loggers = [pylogger]\n",
    "\n",
    "# tensorboard\n",
    "tflogger = TensorBoardLogger(msglogger.logdir, log_1d=True, comment='_'+dataset_name)\n",
    "\n",
    "tflogger.tblogger.writer.add_text('Command line', \"args ---\")\n",
    "\n",
    "msglogger.info('dataset_name:%s\\ndataset_fn=%s\\nnum_classes=%d\\nmodel_name=%s\\seq_len=%s\\nbatch_size=%d\\nvalidation_split=%s\\nlr=%f',\n",
    "                dataset_name,dataset_fn,num_classes,model_name,seq_len,batch_size,validation_split,lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "args = Args(act_mode_8bit=False)\n",
    "train_set, test_set, val_set = dataset_fn((data_path, args), load_train=True, load_test=True, load_val=True)\n",
    "train_set.visualize_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "\ttraining=582237\n",
      "\tvalidation=64692\n",
      "\ttest=161658\n",
      "Augmentations:Compose(\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, _ = apputils.get_data_loaders(\n",
    "        dataset_fn, (data_path,args), batch_size,\n",
    "        workers, validation_split, deterministic,1, 1, 1)\n",
    "msglogger.info('Dataset sizes:\\n\\ttraining=%d\\n\\tvalidation=%d\\n\\ttest=%d',\n",
    "                   len(train_loader.sampler), len(val_loader.sampler), len(test_loader.sampler))\n",
    "msglogger.info('Augmentations:%s',train_loader.dataset.transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Path Initialization\n",
    "pth_path = '../logs/ukdale-train___2024.02.20-031228_golden/'\n",
    "pth_best = os.path.join(os.path.dirname(pth_path), 'cnn1dnilm_best.pth.tar')\n",
    "pth_checkpoint = os.path.join(os.path.dirname(pth_path), 'cnn1dnilm_checkpoint.pth.tar')\n",
    "\n",
    "## Class for Args\n",
    "class Args:\n",
    "    def __init__(self, act_mode_8bit):\n",
    "        self.act_mode_8bit = act_mode_8bit\n",
    "        self.truncate_testset = False\n",
    "\n",
    "args = Args(act_mode_8bit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n",
      "Configuring device: MAX78000, simulate=False.\n"
     ]
    }
   ],
   "source": [
    "## device setup\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n",
    "\n",
    "ai8x.set_device(device=85, simulate=False, round_avg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import ai8x\n",
    "\n",
    "class AI85UNetNILM(nn.Module):\n",
    "    \"\"\"\n",
    "    Large size UNet model. This model also enables the use of folded data.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes=4,         # in_size\n",
    "            num_channels=48,        #\n",
    "            dimensions=(88, 88),  # pylint: disable=unused-argument\n",
    "            dropout=0.1,\n",
    "            bias=True,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.fold_ratio = fold_ratio\n",
    "        # self.num_classes = num_classes\n",
    "        # self.num_final_channels = num_classes * fold_ratio * fold_ratio\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.prep0 = ai8x.FusedConv1dBNReLU(num_channels, 64, 1, stride=1, padding=0,\n",
    "                                            bias=bias, batchnorm='NoAffine', **kwargs)\n",
    "        self.prep1 = ai8x.FusedConv1dBNReLU(64, 64, 1, stride=1, padding=0,\n",
    "                                            bias=bias, batchnorm='NoAffine', **kwargs)\n",
    "        self.prep2 = ai8x.FusedConv1dBNReLU(64, 32, 1, stride=1, padding=0,\n",
    "                                            bias=bias, batchnorm='NoAffine', **kwargs)\n",
    "\n",
    "        self.enc1 = ai8x.FusedConv1dBNReLU(32, 8, 3, stride=1, padding=1,\n",
    "                                           bias=bias, batchnorm='NoAffine', **kwargs)\n",
    "        self.enc2 = ai8x.FusedMaxPoolConv1dBNReLU(8, 28, 3, stride=1, padding=1,\n",
    "                                                  bias=bias, batchnorm='NoAffine', **kwargs)\n",
    "        self.enc3 = ai8x.FusedMaxPoolConv1dBNReLU(28, 56, 3, stride=1, padding=1,\n",
    "                                                  bias=bias, batchnorm='NoAffine', **kwargs)\n",
    "\n",
    "        self.bneck = ai8x.FusedMaxPoolConv1dBNReLU(56, 112, 3, stride=1, padding=1,\n",
    "                                                   bias=bias, batchnorm='NoAffine', **kwargs)\n",
    "\n",
    "        self.upconv3 = ai8x.ConvTranspose2d(112, 56, 3, stride=2, padding=1)\n",
    "        self.dec3 = ai8x.FusedConv1dBNReLU(112, 56, 3, stride=1, padding=1,\n",
    "                                           bias=bias, batchnorm='NoAffine', **kwargs)\n",
    "\n",
    "        # self.upconv2 = nn.ConvTranspose1d(56, 28, 3, stride=2, padding=1)\n",
    "        # self.dec2 = ai8x.FusedConv1dBNReLU(56, 28, 3, stride=1, padding=1,\n",
    "        #                                    bias=bias, batchnorm='NoAffine', **kwargs)\n",
    "\n",
    "        # self.upconv1 = nn.ConvTranspose1d(28, 8, 3, stride=2, padding=1)\n",
    "        # self.dec1 = ai8x.FusedConv1dBNReLU(16, 48, 3, stride=1, padding=1,\n",
    "        #                                    bias=bias, batchnorm='NoAffine', **kwargs)\n",
    "\n",
    "        # self.dec0 = ai8x.FusedConv1dBNReLU(48, 64, 3, stride=1, padding=1,\n",
    "        #                                    bias=bias, batchnorm='NoAffine', **kwargs)\n",
    "\n",
    "        # self.conv_p1 = ai8x.FusedConv1dBNReLU(64, 64, 1, stride=1, padding=0,\n",
    "        #                                       bias=bias, batchnorm='NoAffine', **kwargs)\n",
    "        # self.conv_p2 = ai8x.FusedConv1dBNReLU(64, 64, 1, stride=1, padding=0,\n",
    "        #                                       bias=bias, batchnorm='NoAffine', **kwargs)\n",
    "        # self.conv_p3 = ai8x.Conv1d(64, 64, 1, stride=1, padding=0,\n",
    "        #                                   bias=bias, batchnorm='NoAffine', **kwargs)\n",
    "\n",
    "        # self.conv = ai8x.Conv1d(64, self.num_final_channels, 1, stride=1, padding=0,\n",
    "        #                             bias=bias, batchnorm='NoAffine', **kwargs)\n",
    "\n",
    "    def forward(self, x):  # pylint: disable=arguments-differ\n",
    "        \"\"\"Forward prop\"\"\"\n",
    "        # Run CNN\n",
    "        B = x.size(0)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = self.prep0(x)\n",
    "        # x = self.prep1(x)\n",
    "        # x = self.prep2(x)\n",
    "        \n",
    "        # # Encoder\n",
    "        # enc1 = self.enc1(x)                    # 8x(dim1)x(dim2)\n",
    "        # enc2 = self.enc2(enc1)                 # 28x(dim1/2)x(dim2/2)\n",
    "        # enc3 = self.enc3(enc2)                 # 56x(dim1/4)x(dim2/4)\n",
    "\n",
    "        # bottleneck = self.bneck(enc3)          # 112x(dim1/8)x(dim2/8)\n",
    "\n",
    "        # dec3 = self.upconv3(bottleneck)        # 56x(dim1/4)x(dim2/4)\n",
    "        # dec3 = torch.cat((dec3, enc3), dim=1)  # 112x(dim1/4)x(dim2/4)\n",
    "        # dec3 = self.dec3(dec3)                 # 56x(dim1/4)x(dim2/4)\n",
    "        # dec2 = self.upconv2(dec3)              # 28x(dim1/2)x(dim2/2)\n",
    "        # dec2 = torch.cat((dec2, enc2), dim=1)  # 56(dim1/2)x(dim2/2)\n",
    "        # dec2 = self.dec2(dec2)                 # 28x(dim1/2)x(dim2/2)\n",
    "        # dec1 = self.upconv1(dec2)              # 8x(dim1)x(dim2)\n",
    "        # dec1 = torch.cat((dec1, enc1), dim=1)  # 16x(dim1)x(dim2)\n",
    "        # dec1 = self.dec1(dec1)                 # 48x(dim1)x(dim2)\n",
    "        # dec0 = self.dec0(dec1)                 # 64x(dim1)x(dim2)\n",
    "\n",
    "        # dec0 = self.conv_p1(dec0)\n",
    "        # dec0 = self.conv_p2(dec0)\n",
    "        # dec0 = self.conv_p3(dec0)\n",
    "        # dec0 = self.conv(dec0)                 # num_final_channelsx(dim1)x(dim2)\n",
    "\n",
    "        # return dec0\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class AI85CNN1DNiLM(nn.Module):\n",
    "    def __init__(self, in_size=1, \n",
    "                 output_size=5,\n",
    "                 d_model=64,\n",
    "                 dropout=0.1, \n",
    "                 seq_len=99,  \n",
    "                 n_layers=5, \n",
    "                 n_quantiles=3, \n",
    "                 pool_filter=16,\n",
    "                 device=\"cuda:0\"):\n",
    "        super(AI85CNN1DNiLM, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.enc_net = Encoder(n_channels=in_size, n_kernels=d_model, n_layers=n_layers, seq_size=seq_len, device=device)\n",
    "        self.pool_filter = pool_filter\n",
    "        self.mlp_layer = MLPLayer(in_size=d_model*pool_filter, hidden_arch=[1024], output_size=None)\n",
    "        self.n_quantiles = n_quantiles\n",
    "        \n",
    "        self.fc_out_state  = ai8x.Linear(1024, output_size*2, bias=True)\n",
    "        self.fc_out_power  = ai8x.Linear(1024, output_size*n_quantiles, bias=True)\n",
    "        nn.init.xavier_normal_(self.fc_out_state.op.weight)\n",
    "        nn.init.xavier_normal_(self.fc_out_power.op.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)\n",
    "        B = x.size(0)\n",
    "        print(f\"X: max: {x.max()} min: {x.min()}\")\n",
    "        conv_out = self.dropout(self.enc_net(x))\n",
    "        print(f\"ENC_OUT: max: {conv_out.max()} min: {conv_out.min()}\")\n",
    "        conv_out = F.adaptive_avg_pool1d(conv_out, self.pool_filter)\n",
    "        conv_out = conv_out.reshape(x.size(0), -1)\n",
    "        print(f\"AVG_POOL_OUT: max: {conv_out.max()} min: {conv_out.min()}\")\n",
    "        mlp_out  = self.dropout(self.mlp_layer(conv_out))\n",
    "        print(f\"MLP_OUT: max: {mlp_out.max()} min: {mlp_out.min()}\")\n",
    "        states_logits   = self.fc_out_state(mlp_out).reshape(B, 2, -1)\n",
    "        power_logits    = self.fc_out_power(mlp_out)\n",
    "        if self.n_quantiles>1:\n",
    "            power_logits = power_logits.reshape(B, self.n_quantiles, -1)\n",
    "        print(f\"STATES: max: {states_logits.max()} min: {states_logits.min()}\")\n",
    "        print(f\"POWER: max: {power_logits.max()} min: {power_logits.min()}\")\n",
    "        return states_logits, power_logits\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_channels=10, \n",
    "                 n_kernels=16, \n",
    "                 n_layers=3, \n",
    "                 seq_size=50,\n",
    "                 device=\"cuda:0\"):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.feat_size = (seq_size-1) // 2**n_layers +1\n",
    "        self.feat_dim = self.feat_size * n_kernels\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            *([Conv1D(n_channels, n_kernels // 2**(n_layers-1), activation=\"ReLU\", pooling=\"Max\", last=False, device=device)] +\n",
    "              [Conv1D(n_kernels//2**(n_layers-l),\n",
    "                         n_kernels//2**(n_layers-l-1), activation=\"ReLU\", pooling=\"Max\", last=False, device=device)\n",
    "               for l in range(1, n_layers-1)] +\n",
    "              [Conv1D(n_kernels // 2, n_kernels, activation=\"ReLU\", pooling=\"Max\", last=True, device=device)])\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        assert len(x.size())==3\n",
    "        feats = self.conv_stack(x)\n",
    "        return feats\n",
    "\n",
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, in_size, \n",
    "                 hidden_arch=[128/2, 512/2, 1024/2], \n",
    "                 output_size=None):\n",
    "        \n",
    "        super(MLPLayer, self).__init__()\n",
    "        self.in_size = in_size\n",
    "        self.output_size = output_size\n",
    "        layer_sizes = [in_size] + [x for x in hidden_arch]\n",
    "        self.layers = []\n",
    "        \n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            layer = ai8x.FusedLinearReLU(layer_sizes[i], layer_sizes[i+1])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        if output_size is not None:\n",
    "            layer = ai8x.FusedLinearReLU(layer_sizes[-1], output_size)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.mlp_network =  nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.mlp_network(z)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for layer in self.layers:\n",
    "            try:\n",
    "                if isinstance(layer, ai8x.FusedLinearReLU):\n",
    "                    nn.utils.weight_norm(layer)\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "            except: pass\n",
    "\n",
    "\n",
    "class Conv1D(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_channels,\n",
    "                 num_kernels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 padding=1,\n",
    "                 pooling=\"Max\",\n",
    "                 activation=\"ReLU\",\n",
    "                 batchnorm=\"NoAffine\",\n",
    "                 last=False,\n",
    "                 device=\"cuda:0\",\n",
    "                 **kwargs):\n",
    "        super(Conv1D, self).__init__()\n",
    "        \n",
    "        if not last:\n",
    "            if pooling == \"Max\":\n",
    "                if activation == \"ReLU\":\n",
    "                    self.net = ai8x.FusedMaxPoolConv1dBNReLU(in_channels=num_channels,\n",
    "                                                             out_channels=num_kernels,\n",
    "                                                             kernel_size=kernel_size,\n",
    "                                                             stride=stride,\n",
    "                                                             padding=padding,\n",
    "                                                             bias=True,\n",
    "                                                             batchnorm=\"NoAffine\",\n",
    "                                                             **kwargs)\n",
    "                elif activation == \"Abs\":\n",
    "                    self.net = ai8x.FusedMaxPoolConv1dBNAbs(in_channels=num_channels,\n",
    "                                                            out_channels=num_kernels,\n",
    "                                                            kernel_size=kernel_size,\n",
    "                                                            stride=stride,\n",
    "                                                            padding=padding,\n",
    "                                                            bias=True,\n",
    "                                                            batchnorm=\"NoAffine\",\n",
    "                                                            **kwargs)\n",
    "                else:\n",
    "                    self.net = ai8x.FusedMaxPoolConv1d(in_channels=num_channels,\n",
    "                                                       out_channels=num_kernels,\n",
    "                                                       kernel_size=kernel_size,\n",
    "                                                       stride=stride,\n",
    "                                                       padding=padding,\n",
    "                                                       batchnorm=batchnorm,\n",
    "                                                       **kwargs)\n",
    "            elif pooling == \"Avg\":\n",
    "                if activation == \"ReLU\":\n",
    "                    self.net = ai8x.FusedAvgPoolConv1dBNReLU(in_channels=num_channels,\n",
    "                                                             out_channels=num_kernels,\n",
    "                                                             kernel_size=kernel_size,\n",
    "                                                             stride=stride,\n",
    "                                                             padding=padding,\n",
    "                                                             bias=True,\n",
    "                                                             batchnorm=\"NoAffine\",\n",
    "                                                             **kwargs)\n",
    "                elif activation == \"Abs\":\n",
    "                    self.net = ai8x.FusedAvgPoolConv1dBNAbs(in_channels=num_channels,\n",
    "                                                            out_channels=num_kernels,\n",
    "                                                            kernel_size=kernel_size,\n",
    "                                                            stride=stride,\n",
    "                                                            padding=padding,\n",
    "                                                            bias=True,\n",
    "                                                            batchnorm=\"NoAffine\",\n",
    "                                                            **kwargs)\n",
    "                else:\n",
    "                    self.net = ai8x.FusedAvgPoolConv1d(in_channels=num_channels,\n",
    "                                                       out_channels=num_kernels,\n",
    "                                                       kernel_size=kernel_size,\n",
    "                                                       stride=stride,\n",
    "                                                       padding=padding,\n",
    "                                                       batchnorm=batchnorm,\n",
    "                                                    **kwargs)\n",
    "            else:\n",
    "                if activation == \"ReLU\":\n",
    "                    self.net = ai8x.FusedConv1dBNReLU(in_channels=num_channels,\n",
    "                                                      out_channels=num_kernels,\n",
    "                                                      kernel_size=kernel_size,\n",
    "                                                      stride=stride,\n",
    "                                                      padding=padding,\n",
    "                                                      bias=True,\n",
    "                                                      batchnorm=\"NoAffine\",\n",
    "                                                      **kwargs)\n",
    "                elif activation == \"Abs\":\n",
    "                    self.net = ai8x.FusedConv1dBNAbs(in_channels=num_channels,\n",
    "                                                    out_channels=num_kernels,\n",
    "                                                    kernel_size=kernel_size,\n",
    "                                                    stride=stride,\n",
    "                                                    padding=padding,\n",
    "                                                    bias=True,\n",
    "                                                    batchnorm=\"NoAffine\",\n",
    "                                                    **kwargs)\n",
    "                else:\n",
    "                    self.net = ai8x.FusedAvgPoolConv1d(in_channels=num_channels,\n",
    "                                                       out_channels=num_kernels,\n",
    "                                                       kernel_size=kernel_size,\n",
    "                                                       stride=stride,\n",
    "                                                       padding=padding,\n",
    "                                                       batchnorm=batchnorm,\n",
    "                                                       **kwargs\n",
    "                    )\n",
    "        else:\n",
    "            if pooling == \"Max\":\n",
    "                self.net = ai8x.FusedMaxPoolConv1d(in_channels=num_channels,\n",
    "                                                   out_channels=num_kernels,\n",
    "                                                   kernel_size=kernel_size,\n",
    "                                                   stride=stride,\n",
    "                                                   padding=padding,\n",
    "                                                   batchnorm=batchnorm,\n",
    "                                                   **kwargs\n",
    "                )\n",
    "            elif pooling == \"Avg\":\n",
    "                self.net = ai8x.FusedAvgPoolConv1d(\n",
    "                    in_channels=num_channels,\n",
    "                    out_channels=num_kernels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding,\n",
    "                    batchnorm=batchnorm,\n",
    "                    **kwargs\n",
    "                )\n",
    "            else:\n",
    "                self.net = ai8x.Conv1d(\n",
    "                    in_channels=num_channels,\n",
    "                    out_channels=num_kernels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding,\n",
    "                    batchnorm=batchnorm,\n",
    "                    **kwargs\n",
    "                )\n",
    "\n",
    "        nn.utils.weight_norm(self.net.op.to(device))    \n",
    "        nn.init.xavier_uniform_(self.net.op.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model: AI85CNN1DNiLM(\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (enc_net): Encoder(\n",
      "    (conv_stack): Sequential(\n",
      "      (0): Conv1D(\n",
      "        (net): FusedMaxPoolConv1dBNReLU(\n",
      "          (activate): ReLU(inplace=True)\n",
      "          (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (op): Conv1d(1, 4, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (bn): BatchNorm1d(4, eps=1e-05, momentum=0.05, affine=False, track_running_stats=True)\n",
      "          (calc_out_shift): OutputShiftSqueeze()\n",
      "          (calc_weight_scale): One()\n",
      "          (scale): Scaler()\n",
      "          (calc_out_scale): OutputScale()\n",
      "          (quantize_weight): Empty()\n",
      "          (quantize_bias): Empty()\n",
      "          (clamp_weight): Empty()\n",
      "          (clamp_bias): Empty()\n",
      "          (quantize): Empty()\n",
      "          (clamp): Clamp()\n",
      "          (quantize_pool): Empty()\n",
      "          (clamp_pool): Empty()\n",
      "        )\n",
      "      )\n",
      "      (1): Conv1D(\n",
      "        (net): FusedMaxPoolConv1dBNReLU(\n",
      "          (activate): ReLU(inplace=True)\n",
      "          (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (op): Conv1d(4, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (bn): BatchNorm1d(8, eps=1e-05, momentum=0.05, affine=False, track_running_stats=True)\n",
      "          (calc_out_shift): OutputShiftSqueeze()\n",
      "          (calc_weight_scale): One()\n",
      "          (scale): Scaler()\n",
      "          (calc_out_scale): OutputScale()\n",
      "          (quantize_weight): Empty()\n",
      "          (quantize_bias): Empty()\n",
      "          (clamp_weight): Empty()\n",
      "          (clamp_bias): Empty()\n",
      "          (quantize): Empty()\n",
      "          (clamp): Clamp()\n",
      "          (quantize_pool): Empty()\n",
      "          (clamp_pool): Empty()\n",
      "        )\n",
      "      )\n",
      "      (2): Conv1D(\n",
      "        (net): FusedMaxPoolConv1dBNReLU(\n",
      "          (activate): ReLU(inplace=True)\n",
      "          (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (op): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (bn): BatchNorm1d(16, eps=1e-05, momentum=0.05, affine=False, track_running_stats=True)\n",
      "          (calc_out_shift): OutputShiftSqueeze()\n",
      "          (calc_weight_scale): One()\n",
      "          (scale): Scaler()\n",
      "          (calc_out_scale): OutputScale()\n",
      "          (quantize_weight): Empty()\n",
      "          (quantize_bias): Empty()\n",
      "          (clamp_weight): Empty()\n",
      "          (clamp_bias): Empty()\n",
      "          (quantize): Empty()\n",
      "          (clamp): Clamp()\n",
      "          (quantize_pool): Empty()\n",
      "          (clamp_pool): Empty()\n",
      "        )\n",
      "      )\n",
      "      (3): Conv1D(\n",
      "        (net): FusedMaxPoolConv1dBNReLU(\n",
      "          (activate): ReLU(inplace=True)\n",
      "          (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (op): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (bn): BatchNorm1d(32, eps=1e-05, momentum=0.05, affine=False, track_running_stats=True)\n",
      "          (calc_out_shift): OutputShiftSqueeze()\n",
      "          (calc_weight_scale): One()\n",
      "          (scale): Scaler()\n",
      "          (calc_out_scale): OutputScale()\n",
      "          (quantize_weight): Empty()\n",
      "          (quantize_bias): Empty()\n",
      "          (clamp_weight): Empty()\n",
      "          (clamp_bias): Empty()\n",
      "          (quantize): Empty()\n",
      "          (clamp): Clamp()\n",
      "          (quantize_pool): Empty()\n",
      "          (clamp_pool): Empty()\n",
      "        )\n",
      "      )\n",
      "      (4): Conv1D(\n",
      "        (net): FusedMaxPoolConv1d(\n",
      "          (activate): Empty()\n",
      "          (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (op): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (bn): BatchNorm1d(64, eps=1e-05, momentum=0.05, affine=False, track_running_stats=True)\n",
      "          (calc_out_shift): OutputShiftSqueeze()\n",
      "          (calc_weight_scale): One()\n",
      "          (scale): Scaler()\n",
      "          (calc_out_scale): OutputScale()\n",
      "          (quantize_weight): Empty()\n",
      "          (quantize_bias): Empty()\n",
      "          (clamp_weight): Empty()\n",
      "          (clamp_bias): Empty()\n",
      "          (quantize): Empty()\n",
      "          (clamp): Clamp()\n",
      "          (quantize_pool): Empty()\n",
      "          (clamp_pool): Empty()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mlp_layer): MLPLayer(\n",
      "    (mlp_network): Sequential(\n",
      "      (0): FusedLinearReLU(\n",
      "        (activate): ReLU(inplace=True)\n",
      "        (op): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (calc_out_shift): OutputShiftSqueeze()\n",
      "        (calc_weight_scale): One()\n",
      "        (scale): Scaler()\n",
      "        (calc_out_scale): OutputScale()\n",
      "        (quantize_weight): Empty()\n",
      "        (quantize_bias): Empty()\n",
      "        (clamp_weight): Empty()\n",
      "        (clamp_bias): Empty()\n",
      "        (quantize): Empty()\n",
      "        (clamp): Clamp()\n",
      "        (quantize_pool): Empty()\n",
      "        (clamp_pool): Empty()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc_out_state): Linear(\n",
      "    (activate): Empty()\n",
      "    (op): Linear(in_features=1024, out_features=10, bias=True)\n",
      "    (calc_out_shift): OutputShiftSqueeze()\n",
      "    (calc_weight_scale): One()\n",
      "    (scale): Scaler()\n",
      "    (calc_out_scale): OutputScale()\n",
      "    (quantize_weight): Empty()\n",
      "    (quantize_bias): Empty()\n",
      "    (clamp_weight): Empty()\n",
      "    (clamp_bias): Empty()\n",
      "    (quantize): Empty()\n",
      "    (clamp): Clamp()\n",
      "    (quantize_pool): Empty()\n",
      "    (clamp_pool): Empty()\n",
      "  )\n",
      "  (fc_out_power): Linear(\n",
      "    (activate): Empty()\n",
      "    (op): Linear(in_features=1024, out_features=25, bias=True)\n",
      "    (calc_out_shift): OutputShiftSqueeze()\n",
      "    (calc_weight_scale): One()\n",
      "    (scale): Scaler()\n",
      "    (calc_out_scale): OutputScale()\n",
      "    (quantize_weight): Empty()\n",
      "    (quantize_bias): Empty()\n",
      "    (clamp_weight): Empty()\n",
      "    (clamp_bias): Empty()\n",
      "    (quantize): Empty()\n",
      "    (clamp): Clamp()\n",
      "    (quantize_pool): Empty()\n",
      "    (clamp_pool): Empty()\n",
      "  )\n",
      ")\n",
      "=> loading checkpoint ../logs/ukdale-train___2024.02.20-031228_golden\\cnn1dnilm_checkpoint.pth.tar\n",
      "=> Checkpoint contents:\n",
      "+----------------------+-------------+-----------+\n",
      "| Key                  | Type        | Value     |\n",
      "|----------------------+-------------+-----------|\n",
      "| arch                 | str         | cnn1dnilm |\n",
      "| compression_sched    | dict        |           |\n",
      "| epoch                | int         | 49        |\n",
      "| extras               | dict        |           |\n",
      "| optimizer_state_dict | dict        |           |\n",
      "| optimizer_type       | type        | Adam      |\n",
      "| state_dict           | OrderedDict |           |\n",
      "+----------------------+-------------+-----------+\n",
      "\n",
      "=> Checkpoint['extras'] contents:\n",
      "+-------+--------+---------+\n",
      "| Key   | Type   | Value   |\n",
      "|-------+--------+---------|\n",
      "+-------+--------+---------+\n",
      "\n",
      "Loaded compression schedule from checkpoint (epoch 49)\n",
      "=> loaded 'state_dict' from checkpoint '../logs/ukdale-train___2024.02.20-031228_golden\\cnn1dnilm_checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "model = AI85CNN1DNiLM(in_size=1, output_size=5, dropout=0.4, n_quantiles=len(quantiles), device=device)\n",
    "msglogger.info('model: %s',model)\n",
    "\n",
    "model = apputils.load_lean_checkpoint(model, pth_checkpoint, model_device=device)\n",
    "ai8x.update_model(model)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers = 5\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers)\n",
    "\n",
    "max_in_batch = 0\n",
    "min_in_batch = 100\n",
    "\n",
    "train_batch_with_max = 0\n",
    "test_batch_with_max = 0\n",
    "val_batch_with_max = 0\n",
    "\n",
    "batch_select = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: max: -0.6247190833091736 min: -0.867415726184845\n",
      "ENC_OUT: max: 0.5892009139060974 min: -0.5884112119674683\n",
      "AVG_POOL_OUT: max: 0.5892009139060974 min: -0.5884112119674683\n",
      "MLP_OUT: max: 1.6536457538604736 min: 0.0\n",
      "STATES: max: 0.9921875 min: -1.0\n",
      "POWER: max: 0.9921875 min: -1.0\n"
     ]
    }
   ],
   "source": [
    "index =365\n",
    "data = val_set[index]\n",
    "input_data = data[0]\n",
    "input_data = input_data[None].to(device)\n",
    "states, target = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: max: -0.6247190833091736 min: -0.867415726184845\n",
      "ENC_OUT: max: 0.5892176032066345 min: -0.5877280831336975\n",
      "AVG_POOL_OUT: max: 0.5892176032066345 min: -0.5877280831336975\n",
      "MLP_OUT: max: 1.6536457538604736 min: 0.0\n",
      "STATES: max: 0.9921875 min: -1.0\n",
      "POWER: max: 0.9921875 min: -1.0\n"
     ]
    }
   ],
   "source": [
    "index =365\n",
    "data = val_set[index]\n",
    "input_data = torch.unsqueeze(data[0], dim=0).to(device)\n",
    "states, target = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "753beada7fab6a3ef0fbaf27a665d05d016c9908abab69e0598c961cccc7799f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
