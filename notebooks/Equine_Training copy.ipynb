{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "## Load Libraries and Function Declarations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "from distiller import apputils\n",
    "import ai8x\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "kws20 = importlib.import_module(\"datasets.kws20-horsecough\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support, roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "import librosa\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# FIX SEED FOR REPRODUCIBILITY\n",
    "seed = 69\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "def rescale(audio, min_val=-1,max_val=1):\n",
    "    sig = audio\n",
    "    mean = np.average(sig)\n",
    "\n",
    "    sig = sig-mean # REMOVE DC COMPONENT\n",
    "\n",
    "    sig_max = np.max(sig)\n",
    "    sig_min = np.min(sig)\n",
    "\n",
    "    if sig_max >= np.abs(sig_min):\n",
    "        sig_scaled = sig/sig_max\n",
    "    else:\n",
    "        sig_scaled = sig/np.abs(sig_min)\n",
    "\n",
    "    return sig_scaled\n",
    "\n",
    "def rescale2(audio, min_val=-1,max_val=1):\n",
    "    scaler = MinMaxScaler(feature_range=(min_val,max_val))\n",
    "    audio = audio.reshape(-1,1)\n",
    "    scaler.fit(audio)\n",
    "    scaled = np.array(scaler.transform(audio))\n",
    "    \n",
    "    return scaled[:,0]\n",
    "\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes):\n",
    "    cf_matrix = confusion_matrix(y_true = y_true, y_pred = y_pred, labels =list(range(len(classes))))\n",
    "    print(cf_matrix)\n",
    "\n",
    "def plot_roc_curve(true_y, y_prob):\n",
    "    fpr, tpr, thresholds = roc_curve(true_y, y_prob)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0,1], [0,1], 'r--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "\n",
    "def custom_dataloader(data, train_idx, val_idx, test_idx, batch_size = 2048):\n",
    "    data_file = data\n",
    "    x = np.asarray(data_file[0][test_idx])\n",
    "    y = np.squeeze(np.asarray(data_file[1][test_idx]))\n",
    "    test_set = TensorDataset(torch.Tensor(x),torch.Tensor(y).to(dtype =torch.int64))\n",
    "    test_loader = DataLoader(test_set,batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "    x = np.asarray(data_file[0][val_idx])\n",
    "    y = np.squeeze(np.asarray(data_file[1][val_idx]))\n",
    "    val_set = TensorDataset(torch.Tensor(x),torch.Tensor(y).to(dtype =torch.int64))\n",
    "    val_loader = DataLoader(val_set,batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "    x = np.asarray(data_file[0][train_idx])\n",
    "    y = np.squeeze(np.asarray(data_file[1][train_idx]))\n",
    "    train_set = TensorDataset(torch.Tensor(x),torch.Tensor(y).to(dtype =torch.int64))\n",
    "    train_loader = DataLoader(train_set,batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader,val_loader,test_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_others: 1.0\n",
      "human_cough_v2: 0.98327\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "raw_data_path = Path(\"../data/KWS_EQUINE/raw/\")\n",
    "class_file_count = {}\n",
    "\n",
    "class_dirs = [d for d in raw_data_path.iterdir() if d.is_dir() and d.stem != \"__combinedkws\" and d.stem != \"__human_cough\"]\n",
    "\n",
    "for d in class_dirs:\n",
    "    class_file_count[d] = len(list(d.iterdir()))\n",
    "\n",
    "min_file_count = float(min(class_file_count.values()))\n",
    "\n",
    "for d in class_dirs:\n",
    "    class_file_count[d] = min_file_count / class_file_count[d]\n",
    "    print(f\"{d.stem}: {round(class_file_count[d], 7)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "KWS_HORSE_TF_get_datasets() got an unexpected keyword argument 'download'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# NOTE: Smart compressed file creation\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Change class dicts of main Dataloader class\u001b[39;00m\n\u001b[0;32m      3\u001b[0m train_batch_size \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n\u001b[1;32m----> 4\u001b[0m train_loader, val_loader, test_loader, _ \u001b[39m=\u001b[39m apputils\u001b[39m.\u001b[39;49mget_data_loaders(\n\u001b[0;32m      5\u001b[0m     kws20\u001b[39m.\u001b[39;49mKWS_HORSE_TF_get_datasets, (\u001b[39m\"\u001b[39;49m\u001b[39m../data\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m), train_batch_size, \u001b[39m1\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataset sizes:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mtraining=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_loader\u001b[39m.\u001b[39msampler)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mvalidation=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(val_loader\u001b[39m.\u001b[39msampler)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mtest=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(test_loader\u001b[39m.\u001b[39msampler)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\J_C\\Documents\\GitHub\\ai8x-training\\notebooks\\..\\distiller\\apputils\\data_loaders.py:298\u001b[0m, in \u001b[0;36mget_data_loaders\u001b[1;34m(datasets_fn, data_dir, batch_size, num_workers, validation_split, deterministic, effective_train_size, effective_valid_size, effective_test_size, fixed_subset, sequential, test_only, collate_fn, cpu, download, fname)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_data_loaders\u001b[39m(datasets_fn, data_dir, batch_size, num_workers, validation_split\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, deterministic\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    295\u001b[0m                      effective_train_size\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m, effective_valid_size\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m, effective_test_size\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m, fixed_subset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m                      sequential\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, test_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, collate_fn\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cpu\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, download \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, fname \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mprocessed.pt\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 298\u001b[0m     train_dataset, test_dataset \u001b[39m=\u001b[39m datasets_fn(data_dir, load_train\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m test_only, load_test\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, download \u001b[39m=\u001b[39;49m download, fname \u001b[39m=\u001b[39;49m fname)\n\u001b[0;32m    300\u001b[0m     worker_init_fn \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m deterministic:\n",
      "\u001b[1;31mTypeError\u001b[0m: KWS_HORSE_TF_get_datasets() got an unexpected keyword argument 'download'"
     ]
    }
   ],
   "source": [
    "# NOTE: Smart compressed file creation\n",
    "# Change class dicts of main Dataloader class\n",
    "train_batch_size = 128\n",
    "train_loader, val_loader, test_loader, _ = apputils.get_data_loaders(\n",
    "    kws20.KWS_HORSE_TF_get_datasets, (\"../data\", False), train_batch_size, 1, validation_split=0.1)\n",
    "\n",
    "print(f\"Dataset sizes:\\n\\ttraining={len(train_loader.sampler)}\\n\\tvalidation={len(val_loader.sampler)}\\n\\ttest={len(test_loader.sampler)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['combined','horse_cough']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring device: MAX78000, simulate=False.\n",
      "Number of Model Params: 173568\n"
     ]
    }
   ],
   "source": [
    "ai8x.set_device(device=85, simulate=False, round_avg=False)\n",
    "\n",
    "mod = importlib.import_module(\"models.ai85net-kws20-kabayo\")\n",
    "\n",
    "model = mod.AI85KWS20Netv3(num_classes=21, num_channels=128, dimensions=(128, 1), bias=False)\n",
    "print(f'Number of Model Params: {count_params(model)}')\n",
    "\n",
    "# model, compression_scheduler, optimizer, start_epoch = apputils.load_checkpoint(\n",
    "#             model, \"logs/2023.01.10-013008/qat_best.pth.tar\", model_device='cuda')\n",
    "\n",
    "model, compression_scheduler, optimizer, start_epoch = apputils.load_checkpoint(\n",
    "            model, \"../logs/kws20/qat_best.pth.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace FC layer and freeze the rest of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layer(layer):\n",
    "    for p in layer.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_layer(model.voice_conv1)\n",
    "freeze_layer(model.voice_conv2)\n",
    "freeze_layer(model.voice_conv3)\n",
    "freeze_layer(model.voice_conv4)\n",
    "freeze_layer(model.kws_conv1)\n",
    "# freeze_layer(model.kws_conv2)\n",
    "# freeze_layer(model.kws_conv3)\n",
    "# freeze_layer(model.kws_conv4)\n",
    "model.fc = ai8x.Linear(256, len(classes), bias=False, wide=True)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "epoch = 0\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "ms_lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 80], gamma=0.5)\n",
    "# criterion = torch.nn.CrossEntropyLoss(\n",
    "#     torch.Tensor((1, 1, 1))\n",
    "# )\n",
    "# criterion.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(\n",
    "    torch.Tensor((1, 1))\n",
    ")\n",
    "criterion.to(device)\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss(\n",
    "#     torch.Tensor((1, 1, 0.01, 1, 1, 0.01, 0.01, 0.01))\n",
    "# )\n",
    "# criterion.to(device)\n",
    "\n",
    "qat_policy = {\n",
    "    'start_epoch': 20,\n",
    "    'weight_bits': 8\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes):\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred, list(range(len(classes))))\n",
    "    print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (input, target) in enumerate(train_loader):\n",
    "#     print(np.shape(input), np.shape(target), sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50\t LR: [0.0001]\t Train Loss: 0.6912\t Dur: 11.03 sec.\n",
      "Best model saved with accuracy: 76.67%\n",
      "\t\t Test Acc: 76.67\n",
      "\t\tConfusion:\n",
      "[[ 0  6]\n",
      " [ 1 23]]\n",
      "Epoch: 2/50\t LR: [0.0001]\t Train Loss: 0.6823\t Dur: 10.50 sec.\n",
      "\t\t Test Acc: 76.67\n",
      "\t\tConfusion:\n",
      "[[ 0  6]\n",
      " [ 1 23]]\n",
      "Epoch: 3/50\t LR: [0.0001]\t Train Loss: 0.6639\t Dur: 10.55 sec.\n",
      "Best model saved with accuracy: 80.00%\n",
      "\t\t Test Acc: 80.00\n",
      "\t\tConfusion:\n",
      "[[ 1  5]\n",
      " [ 1 23]]\n",
      "Epoch: 4/50\t LR: [0.0001]\t Train Loss: 0.6553\t Dur: 10.40 sec.\n",
      "Best model saved with accuracy: 83.33%\n",
      "\t\t Test Acc: 83.33\n",
      "\t\tConfusion:\n",
      "[[ 2  4]\n",
      " [ 1 23]]\n",
      "Epoch: 5/50\t LR: [0.0001]\t Train Loss: 0.6532\t Dur: 10.69 sec.\n",
      "\t\t Test Acc: 80.00\n",
      "\t\tConfusion:\n",
      "[[ 2  4]\n",
      " [ 2 22]]\n",
      "Epoch: 6/50\t LR: [0.0001]\t Train Loss: 0.6287\t Dur: 11.13 sec.\n",
      "\t\t Test Acc: 80.00\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 3 21]]\n",
      "Epoch: 7/50\t LR: [0.0001]\t Train Loss: 0.6414\t Dur: 10.39 sec.\n",
      "\t\t Test Acc: 83.33\n",
      "\t\tConfusion:\n",
      "[[ 4  2]\n",
      " [ 3 21]]\n",
      "Epoch: 8/50\t LR: [0.0001]\t Train Loss: 0.6336\t Dur: 10.92 sec.\n",
      "\t\t Test Acc: 76.67\n",
      "\t\tConfusion:\n",
      "[[ 4  2]\n",
      " [ 5 19]]\n",
      "Epoch: 9/50\t LR: [0.0001]\t Train Loss: 0.6340\t Dur: 11.80 sec.\n",
      "\t\t Test Acc: 70.00\n",
      "\t\tConfusion:\n",
      "[[ 4  2]\n",
      " [ 7 17]]\n",
      "Epoch: 10/50\t LR: [0.0001]\t Train Loss: 0.6184\t Dur: 10.57 sec.\n",
      "\t\t Test Acc: 63.33\n",
      "\t\tConfusion:\n",
      "[[ 5  1]\n",
      " [10 14]]\n",
      "Epoch: 11/50\t LR: [0.0001]\t Train Loss: 0.6008\t Dur: 10.39 sec.\n",
      "\t\t Test Acc: 63.33\n",
      "\t\tConfusion:\n",
      "[[ 5  1]\n",
      " [10 14]]\n",
      "Epoch: 12/50\t LR: [0.0001]\t Train Loss: 0.6062\t Dur: 11.14 sec.\n",
      "\t\t Test Acc: 63.33\n",
      "\t\tConfusion:\n",
      "[[ 6  0]\n",
      " [11 13]]\n",
      "Epoch: 13/50\t LR: [0.0001]\t Train Loss: 0.6175\t Dur: 10.66 sec.\n",
      "\t\t Test Acc: 63.33\n",
      "\t\tConfusion:\n",
      "[[ 6  0]\n",
      " [11 13]]\n",
      "Epoch: 14/50\t LR: [0.0001]\t Train Loss: 0.5815\t Dur: 10.71 sec.\n",
      "\t\t Test Acc: 63.33\n",
      "\t\tConfusion:\n",
      "[[ 6  0]\n",
      " [11 13]]\n",
      "Epoch: 15/50\t LR: [0.0001]\t Train Loss: 0.6041\t Dur: 10.25 sec.\n",
      "\t\t Test Acc: 63.33\n",
      "\t\tConfusion:\n",
      "[[ 6  0]\n",
      " [11 13]]\n",
      "Epoch: 16/50\t LR: [0.0001]\t Train Loss: 0.5823\t Dur: 10.68 sec.\n",
      "\t\t Test Acc: 63.33\n",
      "\t\tConfusion:\n",
      "[[ 6  0]\n",
      " [11 13]]\n",
      "Epoch: 17/50\t LR: [0.0001]\t Train Loss: 0.5785\t Dur: 10.68 sec.\n",
      "\t\t Test Acc: 63.33\n",
      "\t\tConfusion:\n",
      "[[ 6  0]\n",
      " [11 13]]\n",
      "Epoch: 18/50\t LR: [0.0001]\t Train Loss: 0.5954\t Dur: 11.09 sec.\n",
      "\t\t Test Acc: 63.33\n",
      "\t\tConfusion:\n",
      "[[ 6  0]\n",
      " [11 13]]\n",
      "Epoch: 19/50\t LR: [0.0001]\t Train Loss: 0.5956\t Dur: 10.26 sec.\n",
      "\t\t Test Acc: 60.00\n",
      "\t\tConfusion:\n",
      "[[ 6  0]\n",
      " [12 12]]\n",
      "Epoch: 20/50\t LR: [0.0001]\t Train Loss: 0.5820\t Dur: 10.66 sec.\n",
      "\t\t Test Acc: 60.00\n",
      "\t\tConfusion:\n",
      "[[ 6  0]\n",
      " [12 12]]\n",
      "QAT is starting!\n",
      "Epoch: 21/50\t LR: [2.5e-05]\t Train Loss: 1.3337\t Dur: 10.86 sec.\n",
      "Best model saved with accuracy: 43.33%\n",
      "\t\t Test Acc: 43.33\n",
      "\t\tConfusion:\n",
      "[[ 6  0]\n",
      " [17  7]]\n",
      "Epoch: 22/50\t LR: [5e-05]\t Train Loss: 1.2894\t Dur: 11.04 sec.\n",
      "Best model saved with accuracy: 46.67%\n",
      "\t\t Test Acc: 46.67\n",
      "\t\tConfusion:\n",
      "[[ 6  0]\n",
      " [16  8]]\n",
      "Epoch: 23/50\t LR: [5e-05]\t Train Loss: 1.2082\t Dur: 12.97 sec.\n",
      "Best model saved with accuracy: 50.00%\n",
      "\t\t Test Acc: 50.00\n",
      "\t\tConfusion:\n",
      "[[ 6  0]\n",
      " [15  9]]\n",
      "Epoch: 24/50\t LR: [5e-05]\t Train Loss: 1.0406\t Dur: 12.10 sec.\n",
      "\t\t Test Acc: 46.67\n",
      "\t\tConfusion:\n",
      "[[ 5  1]\n",
      " [15  9]]\n",
      "Epoch: 25/50\t LR: [5e-05]\t Train Loss: 1.0575\t Dur: 11.63 sec.\n",
      "\t\t Test Acc: 50.00\n",
      "\t\tConfusion:\n",
      "[[ 5  1]\n",
      " [14 10]]\n",
      "Epoch: 26/50\t LR: [5e-05]\t Train Loss: 0.8551\t Dur: 11.38 sec.\n",
      "Best model saved with accuracy: 60.00%\n",
      "\t\t Test Acc: 60.00\n",
      "\t\tConfusion:\n",
      "[[ 5  1]\n",
      " [11 13]]\n",
      "Epoch: 27/50\t LR: [5e-05]\t Train Loss: 0.8661\t Dur: 11.39 sec.\n",
      "\t\t Test Acc: 60.00\n",
      "\t\tConfusion:\n",
      "[[ 5  1]\n",
      " [11 13]]\n",
      "Epoch: 28/50\t LR: [5e-05]\t Train Loss: 0.7598\t Dur: 11.02 sec.\n",
      "Best model saved with accuracy: 66.67%\n",
      "\t\t Test Acc: 66.67\n",
      "\t\tConfusion:\n",
      "[[ 5  1]\n",
      " [ 9 15]]\n",
      "Epoch: 29/50\t LR: [5e-05]\t Train Loss: 0.7580\t Dur: 12.93 sec.\n",
      "Best model saved with accuracy: 70.00%\n",
      "\t\t Test Acc: 70.00\n",
      "\t\tConfusion:\n",
      "[[ 5  1]\n",
      " [ 8 16]]\n",
      "Epoch: 30/50\t LR: [5e-05]\t Train Loss: 0.6094\t Dur: 11.89 sec.\n",
      "\t\t Test Acc: 70.00\n",
      "\t\tConfusion:\n",
      "[[ 5  1]\n",
      " [ 8 16]]\n",
      "Epoch: 31/50\t LR: [5e-05]\t Train Loss: 0.5753\t Dur: 11.64 sec.\n",
      "Best model saved with accuracy: 76.67%\n",
      "\t\t Test Acc: 76.67\n",
      "\t\tConfusion:\n",
      "[[ 4  2]\n",
      " [ 5 19]]\n",
      "Epoch: 32/50\t LR: [5e-05]\t Train Loss: 0.5838\t Dur: 11.22 sec.\n",
      "\t\t Test Acc: 76.67\n",
      "\t\tConfusion:\n",
      "[[ 4  2]\n",
      " [ 5 19]]\n",
      "Epoch: 33/50\t LR: [5e-05]\t Train Loss: 0.4702\t Dur: 10.70 sec.\n",
      "\t\t Test Acc: 73.33\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 5 19]]\n",
      "Epoch: 34/50\t LR: [5e-05]\t Train Loss: 0.5388\t Dur: 10.78 sec.\n",
      "\t\t Test Acc: 73.33\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 5 19]]\n",
      "Epoch: 35/50\t LR: [5e-05]\t Train Loss: 0.4702\t Dur: 10.74 sec.\n",
      "\t\t Test Acc: 73.33\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 5 19]]\n",
      "Epoch: 36/50\t LR: [5e-05]\t Train Loss: 0.5134\t Dur: 10.86 sec.\n",
      "Best model saved with accuracy: 80.00%\n",
      "\t\t Test Acc: 80.00\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 3 21]]\n",
      "Epoch: 37/50\t LR: [5e-05]\t Train Loss: 0.5256\t Dur: 10.76 sec.\n",
      "Best model saved with accuracy: 83.33%\n",
      "\t\t Test Acc: 83.33\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 2 22]]\n",
      "Epoch: 38/50\t LR: [5e-05]\t Train Loss: 0.4377\t Dur: 10.82 sec.\n",
      "\t\t Test Acc: 83.33\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 2 22]]\n",
      "Epoch: 39/50\t LR: [5e-05]\t Train Loss: 0.5161\t Dur: 10.91 sec.\n",
      "\t\t Test Acc: 83.33\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 2 22]]\n",
      "Epoch: 40/50\t LR: [5e-05]\t Train Loss: 0.4474\t Dur: 11.91 sec.\n",
      "Best model saved with accuracy: 86.67%\n",
      "\t\t Test Acc: 86.67\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 1 23]]\n",
      "Epoch: 41/50\t LR: [5e-05]\t Train Loss: 0.4829\t Dur: 11.74 sec.\n",
      "\t\t Test Acc: 86.67\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 1 23]]\n",
      "Epoch: 42/50\t LR: [5e-05]\t Train Loss: 0.4500\t Dur: 10.90 sec.\n",
      "\t\t Test Acc: 86.67\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 1 23]]\n",
      "Epoch: 43/50\t LR: [5e-05]\t Train Loss: 0.4348\t Dur: 11.30 sec.\n",
      "\t\t Test Acc: 86.67\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 1 23]]\n",
      "Epoch: 44/50\t LR: [5e-05]\t Train Loss: 0.4438\t Dur: 11.00 sec.\n",
      "\t\t Test Acc: 86.67\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 1 23]]\n",
      "Epoch: 45/50\t LR: [5e-05]\t Train Loss: 0.4245\t Dur: 10.75 sec.\n",
      "\t\t Test Acc: 86.67\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 1 23]]\n",
      "Epoch: 46/50\t LR: [5e-05]\t Train Loss: 0.3841\t Dur: 10.77 sec.\n",
      "\t\t Test Acc: 86.67\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 1 23]]\n",
      "Epoch: 47/50\t LR: [5e-05]\t Train Loss: 0.4206\t Dur: 10.98 sec.\n",
      "\t\t Test Acc: 83.33\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 2 22]]\n",
      "Epoch: 48/50\t LR: [5e-05]\t Train Loss: 0.4264\t Dur: 10.61 sec.\n",
      "\t\t Test Acc: 83.33\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 2 22]]\n",
      "Epoch: 49/50\t LR: [5e-05]\t Train Loss: 0.3064\t Dur: 12.55 sec.\n",
      "\t\t Test Acc: 83.33\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 2 22]]\n",
      "Epoch: 50/50\t LR: [5e-05]\t Train Loss: 0.3568\t Dur: 11.83 sec.\n",
      "\t\t Test Acc: 83.33\n",
      "\t\tConfusion:\n",
      "[[ 3  3]\n",
      " [ 2 22]]\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_qat_acc = 0\n",
    "for epoch in range(0, num_epochs):\n",
    "    if epoch > 0 and epoch == qat_policy['start_epoch']:\n",
    "        print('QAT is starting!')\n",
    "        # Fuse the BN parameters into conv layers before Quantization Aware Training (QAT)\n",
    "        ai8x.fuse_bn_layers(model)\n",
    "\n",
    "        # Switch model from unquantized to quantized for QAT\n",
    "        ai8x.initiate_qat(model, qat_policy)\n",
    "\n",
    "        # Model is re-transferred to GPU in case parameters were added\n",
    "        model.to(device)\n",
    "    running_loss = []\n",
    "    train_start = time.time()\n",
    "    model.train()\n",
    "    for idx, (inputs, target) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model_out = model(inputs)\n",
    "        \n",
    "        loss = criterion(model_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "    mean_loss = np.mean(running_loss)\n",
    "    train_end = time.time()\n",
    "    print(\"Epoch: {}/{}\\t LR: {}\\t Train Loss: {:.4f}\\t Dur: {:.2f} sec.\".format(\n",
    "        epoch+1, num_epochs, ms_lr_scheduler.get_lr(), mean_loss, (train_end-train_start)))\n",
    "    \n",
    "    model.eval()\n",
    "    acc = 0.\n",
    "    acc_weight = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            model_out = model(inputs)\n",
    "            target_out = torch.argmax(model_out, dim=1)\n",
    "            \n",
    "            y_pred.extend(target_out.cpu().numpy())\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            \n",
    "            tp = torch.sum(target_out == target)\n",
    "            acc_batch = (tp / target_out.numel()).detach().item()\n",
    "            acc += target_out.shape[0] * acc_batch\n",
    "            acc_weight += target_out.shape[0]\n",
    "            \n",
    "        total_acc = 100 * (acc / acc_weight)\n",
    "        if epoch == qat_policy['start_epoch']: best_acc = 0\n",
    "        if total_acc > best_acc:\n",
    "            best_acc = total_acc\n",
    "            checkpoint_extras = {'current_top1': best_acc,\n",
    "                                 'best_top1': best_acc,\n",
    "                                 'best_epoch': epoch}\n",
    "            model_name = 'ai85net_kws_equine'\n",
    "            model_prefix = f'{model_name}' if epoch < qat_policy['start_epoch'] else (f'qat_{model_name}')\n",
    "            apputils.save_checkpoint(epoch, model_name, model, optimizer=optimizer,\n",
    "                                     scheduler=None, extras=checkpoint_extras,\n",
    "                                     is_best=True, name=model_prefix,\n",
    "                                     dir='.')\n",
    "            print(f'Best model saved with accuracy: {best_acc:.2f}%')\n",
    "            \n",
    "        print('\\t\\t Test Acc: {:.2f}'.format(total_acc))\n",
    "        print(\"\\t\\tConfusion:\")\n",
    "        plot_confusion(y_true, y_pred, classes)\n",
    "    ms_lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33335497366de2c65b60bad2619bdb398d3af58569807f2e4d877bfe78d46b65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
