{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "## Load Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "from distiller import apputils\n",
    "import ai8x\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "kws20 = importlib.import_module(\"datasets.kws20-horsecough\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion(y_true, y_pred, classes):\n",
    "    cf_matrix = confusion_matrix(y_true = y_true, y_pred = y_pred, labels =list(range(len(classes))))\n",
    "    print(cf_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating combined Dataset. Class Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# #raw_data_path = Path(\"C:/Users/J_C/Desktop/DATASETS/raw\")\n",
    "# raw_data_path = Path(\"C:/Users/J_C/Desktop/DATASETS/raw/\")\n",
    "# class_file_count = {}\n",
    "\n",
    "# class_dirs = [d for d in raw_data_path.iterdir() if d.is_dir() and (d.stem != \"_background_noise_\")]\n",
    "\n",
    "# # Create combined Dataset\n",
    "# import shutil\n",
    "# combined_path = \"C:/Users/J_C/Documents/GitHub/ai8x-training/data/KWS_EQUINE/combined/\"\n",
    "# if os.path.isdir(combined_path) == False:\n",
    "#     os.makedirs(combined_path)\n",
    "\n",
    "# for i,d in enumerate(class_dirs):\n",
    "#     print(class_file_count[d] ,\" files in folder \",d)\n",
    "#     fnames =  os.listdir(d)\n",
    "#     for fi,f in enumerate(d.iterdir()):\n",
    "#         shutil.copyfile(f, combined_path+'/'+str(i)+'_'+fnames[fi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined: 0.0171171\n",
      "human_cough: 1.0\n",
      "Weights:  [0.0171171, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#raw_data_path = Path(\"C:/Users/J_C/Desktop/DATASETS/raw\")\n",
    "raw_data_path = Path(\"../data/KWS_EQUINE/raw/\")\n",
    "class_file_count = {}\n",
    "\n",
    "class_dirs = [d for d in raw_data_path.iterdir() if d.is_dir() and (d.stem != \"_background_noise_\")]\n",
    "\n",
    "for d in class_dirs:\n",
    "    class_file_count[d] = len(list(d.iterdir()))\n",
    "\n",
    "min_file_count = float(min(class_file_count.values()))\n",
    "\n",
    "# Calculate weights\n",
    "class_weights = []\n",
    "for d in class_dirs:\n",
    "    class_file_count[d] = min_file_count / class_file_count[d]\n",
    "    print(f\"{d.stem}: {round(class_file_count[d], 7)}\")\n",
    "    class_weights.append(round(class_file_count[d], 7))\n",
    "\n",
    "print('Weights: ',class_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No key `noise_var` in input augmentation dictionary!  Using defaults: [Min: 0., Max: 1.]\n",
      "No key `shift` in input augmentation dictionary! Using defaults: [Min:-0.1, Max: 0.1]\n",
      "No key `strech` in input augmentation dictionary! Using defaults: [Min: 0.8, Max: 1.3]\n",
      "Generating dataset from raw data samples for the first time. \n",
      "This process will take significant time (~60 minutes)...\n",
      "data_len: 16384\n",
      "------------- Label Size ---------------\n",
      "combined:  \t219664\n",
      "human_cough:  \t3760\n",
      "------------------------------------------\n",
      "Processing the label: combined. 1 of 2\n",
      "\t1 of 219664\n",
      "\t1001 of 219664\n",
      "\t2001 of 219664\n",
      "\t3001 of 219664\n",
      "\t4001 of 219664\n",
      "\t5001 of 219664\n",
      "\t6001 of 219664\n",
      "\t7001 of 219664\n",
      "\t8001 of 219664\n",
      "\t9001 of 219664\n",
      "\t10001 of 219664\n",
      "\t11001 of 219664\n",
      "\t12001 of 219664\n",
      "\t13001 of 219664\n",
      "\t14001 of 219664\n",
      "\t15001 of 219664\n",
      "\t16001 of 219664\n",
      "\t17001 of 219664\n",
      "\t18001 of 219664\n",
      "\t19001 of 219664\n",
      "\t20001 of 219664\n",
      "\t21001 of 219664\n",
      "\t22001 of 219664\n",
      "\t23001 of 219664\n",
      "\t24001 of 219664\n",
      "\t25001 of 219664\n",
      "\t26001 of 219664\n",
      "\t27001 of 219664\n",
      "\t28001 of 219664\n",
      "\t29001 of 219664\n",
      "\t30001 of 219664\n",
      "\t31001 of 219664\n",
      "\t32001 of 219664\n",
      "\t33001 of 219664\n",
      "\t34001 of 219664\n",
      "\t35001 of 219664\n",
      "\t36001 of 219664\n",
      "\t37001 of 219664\n",
      "\t38001 of 219664\n",
      "\t39001 of 219664\n",
      "\t40001 of 219664\n",
      "\t41001 of 219664\n",
      "\t42001 of 219664\n",
      "\t43001 of 219664\n",
      "\t44001 of 219664\n",
      "\t45001 of 219664\n",
      "\t46001 of 219664\n",
      "\t47001 of 219664\n",
      "\t48001 of 219664\n",
      "\t49001 of 219664\n",
      "\t50001 of 219664\n",
      "\t51001 of 219664\n",
      "\t52001 of 219664\n",
      "\t53001 of 219664\n",
      "\t54001 of 219664\n",
      "\t55001 of 219664\n",
      "\t56001 of 219664\n",
      "\t57001 of 219664\n",
      "\t58001 of 219664\n",
      "\t59001 of 219664\n",
      "\t60001 of 219664\n",
      "\t61001 of 219664\n",
      "\t62001 of 219664\n",
      "\t63001 of 219664\n",
      "\t64001 of 219664\n",
      "\t65001 of 219664\n",
      "\t66001 of 219664\n",
      "\t67001 of 219664\n",
      "\t68001 of 219664\n",
      "\t69001 of 219664\n",
      "\t70001 of 219664\n",
      "\t71001 of 219664\n",
      "\t72001 of 219664\n",
      "\t73001 of 219664\n",
      "\t74001 of 219664\n",
      "\t75001 of 219664\n",
      "\t76001 of 219664\n",
      "\t77001 of 219664\n",
      "\t78001 of 219664\n",
      "\t79001 of 219664\n",
      "\t80001 of 219664\n",
      "\t81001 of 219664\n",
      "\t82001 of 219664\n",
      "\t83001 of 219664\n",
      "\t84001 of 219664\n",
      "\t85001 of 219664\n",
      "\t86001 of 219664\n",
      "\t87001 of 219664\n",
      "\t88001 of 219664\n",
      "Gen dataset error:  PySoundFile failed. Trying audioread instead.\n",
      "\t89001 of 219664\n",
      "\t90001 of 219664\n",
      "\t91001 of 219664\n",
      "\t92001 of 219664\n",
      "\t93001 of 219664\n",
      "\t94001 of 219664\n",
      "\t95001 of 219664\n",
      "\t96001 of 219664\n",
      "\t97001 of 219664\n",
      "\t98001 of 219664\n",
      "\t99001 of 219664\n",
      "\t100001 of 219664\n",
      "\t101001 of 219664\n",
      "\t102001 of 219664\n",
      "\t103001 of 219664\n",
      "\t104001 of 219664\n",
      "\t105001 of 219664\n",
      "\t106001 of 219664\n",
      "\t107001 of 219664\n",
      "\t108001 of 219664\n",
      "\t109001 of 219664\n",
      "\t110001 of 219664\n",
      "\t111001 of 219664\n",
      "\t112001 of 219664\n",
      "\t113001 of 219664\n",
      "\t114001 of 219664\n",
      "\t115001 of 219664\n",
      "\t116001 of 219664\n",
      "\t117001 of 219664\n",
      "\t118001 of 219664\n",
      "\t119001 of 219664\n",
      "\t120001 of 219664\n",
      "\t121001 of 219664\n",
      "\t122001 of 219664\n",
      "\t123001 of 219664\n",
      "\t124001 of 219664\n",
      "\t125001 of 219664\n",
      "\t126001 of 219664\n",
      "\t127001 of 219664\n",
      "\t128001 of 219664\n",
      "\t129001 of 219664\n",
      "\t130001 of 219664\n",
      "\t131001 of 219664\n",
      "\t132001 of 219664\n",
      "\t133001 of 219664\n",
      "\t134001 of 219664\n",
      "\t135001 of 219664\n",
      "\t136001 of 219664\n",
      "\t137001 of 219664\n",
      "\t138001 of 219664\n",
      "\t139001 of 219664\n",
      "\t140001 of 219664\n",
      "\t141001 of 219664\n",
      "\t142001 of 219664\n",
      "\t143001 of 219664\n",
      "\t144001 of 219664\n",
      "\t145001 of 219664\n",
      "\t146001 of 219664\n",
      "\t147001 of 219664\n",
      "\t148001 of 219664\n",
      "\t149001 of 219664\n",
      "\t150001 of 219664\n",
      "\t151001 of 219664\n",
      "\t152001 of 219664\n",
      "\t153001 of 219664\n",
      "\t154001 of 219664\n",
      "\t155001 of 219664\n",
      "\t156001 of 219664\n",
      "\t157001 of 219664\n",
      "\t158001 of 219664\n",
      "\t159001 of 219664\n",
      "\t160001 of 219664\n",
      "\t161001 of 219664\n",
      "\t162001 of 219664\n",
      "\t163001 of 219664\n",
      "\t164001 of 219664\n",
      "\t165001 of 219664\n",
      "\t166001 of 219664\n",
      "\t167001 of 219664\n",
      "\t168001 of 219664\n",
      "\t169001 of 219664\n",
      "\t170001 of 219664\n",
      "\t171001 of 219664\n",
      "\t172001 of 219664\n",
      "\t173001 of 219664\n",
      "\t174001 of 219664\n",
      "\t175001 of 219664\n",
      "\t176001 of 219664\n",
      "\t177001 of 219664\n",
      "\t178001 of 219664\n",
      "\t179001 of 219664\n",
      "\t180001 of 219664\n",
      "\t181001 of 219664\n",
      "\t182001 of 219664\n",
      "\t183001 of 219664\n",
      "\t184001 of 219664\n",
      "\t185001 of 219664\n",
      "\t186001 of 219664\n",
      "\t187001 of 219664\n",
      "\t188001 of 219664\n",
      "\t189001 of 219664\n",
      "\t190001 of 219664\n",
      "\t191001 of 219664\n",
      "\t192001 of 219664\n",
      "\t193001 of 219664\n",
      "\t194001 of 219664\n",
      "\t195001 of 219664\n",
      "\t196001 of 219664\n",
      "\t197001 of 219664\n",
      "\t198001 of 219664\n",
      "\t199001 of 219664\n",
      "\t200001 of 219664\n",
      "\t201001 of 219664\n",
      "\t202001 of 219664\n",
      "\t203001 of 219664\n",
      "\t204001 of 219664\n",
      "\t205001 of 219664\n",
      "\t206001 of 219664\n",
      "\t207001 of 219664\n",
      "\t208001 of 219664\n",
      "\t209001 of 219664\n",
      "\t210001 of 219664\n",
      "\t211001 of 219664\n",
      "\t212001 of 219664\n",
      "\t213001 of 219664\n",
      "\t214001 of 219664\n",
      "\t215001 of 219664\n",
      "\t216001 of 219664\n",
      "\t217001 of 219664\n",
      "\t218001 of 219664\n",
      "\t219001 of 219664\n",
      "Finished in 4134.152 seconds.\n",
      "(658992, 128, 128)\n",
      "Data concatenation finished in 3.041 seconds.\n",
      "Processing the label: human_cough. 2 of 2\n",
      "\t1 of 3760\n",
      "\t1001 of 3760\n",
      "\t2001 of 3760\n",
      "\t3001 of 3760\n",
      "Finished in 73.870 seconds.\n",
      "(11280, 128, 128)\n",
      "Data concatenation finished in 1.659 seconds.\n",
      "Dataset created.\n",
      "Training+Validation: 3383,  Test: 377\n",
      "\n",
      "Processing train...\n",
      "Class combined not found in data\n",
      "\n",
      "Processing test...\n",
      "Class combined not found in data\n",
      "Dataset sizes:\n",
      "\ttraining=542606\n",
      "\tvalidation=60289\n",
      "\ttest=67374\n",
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Smart compressed file creation\n",
    "# Change class dicts of main Dataloader class\n",
    "train_batch_size = 256\n",
    "train_loader, val_loader, test_loader, _ = apputils.get_data_loaders(\n",
    "    kws20.KWS_HORSE_TF_get_datasets, (\"../data\", True), train_batch_size, 1, validation_split=0.1)\n",
    "\n",
    "print(f\"Dataset sizes:\\n\\ttraining={len(train_loader.sampler)}\\n\\tvalidation={len(val_loader.sampler)}\\n\\ttest={len(test_loader.sampler)}\")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n",
    "\n",
    "classes = [\"combined\",'human_cough']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Reference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring device: MAX78000, simulate=False.\n",
      "169472\n",
      "Number of Model Params: 169472\n"
     ]
    }
   ],
   "source": [
    "def count_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    print(params)\n",
    "    return params\n",
    "\n",
    "ai8x.set_device(device=85, simulate=False, round_avg=False)\n",
    "\n",
    "mod = importlib.import_module(\"models.ai85net-kws20-v3\")\n",
    "\n",
    "model = mod.AI85KWS20Netv3(num_classes=21, num_channels=128, dimensions=(128, 1), bias=False)\n",
    "print(f'Number of Model Params: {count_params(model)}')\n",
    "\n",
    "# WEIGHTS OF REFERENCE MODEL\n",
    "model, compression_scheduler, optimizer, start_epoch = apputils.load_checkpoint(\n",
    "            model, \"../logs/kws20_original/qat_best.pth.tar\")\n",
    "\n",
    " # FREEZE SOME LAYERS\n",
    "def freeze_layer(layer):\n",
    "    for p in layer.parameters():\n",
    "        p.requires_grad = False\n",
    "freeze_layer(model.voice_conv1)\n",
    "freeze_layer(model.voice_conv2)\n",
    "freeze_layer(model.voice_conv3)\n",
    "freeze_layer(model.voice_conv4)\n",
    "freeze_layer(model.kws_conv1)\n",
    "freeze_layer(model.kws_conv2)\n",
    "# freeze_layer(model.kws_conv3)\n",
    "# freeze_layer(model.kws_conv4)\n",
    "model.fc = ai8x.Linear(256, len(classes), bias=False, wide=True)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "ms_lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(num_epochs/4), int(num_epochs/2)], gamma=0.2)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights))\n",
    "criterion.to(device)\n",
    "\n",
    "qat_policy = {\n",
    "    'start_epoch': int(num_epochs/5),\n",
    "    'weight_bits': 8\n",
    "    }\n",
    "\n",
    "model_name = 'human_kws20_1'\n",
    "checkpoint_dir = './checkpoints/'+model_name+'/'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "plot_dir ='./checkpoints/plots/'+model_name+'/'\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m train_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     25\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfor\u001b[39;00m idx, (inputs, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     27\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     28\u001b[0m     target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\J_C\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\J_C\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1358\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1359\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1361\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1362\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\J_C\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1315\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m   1314\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[1;32m-> 1315\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1316\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1317\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\J_C\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1151\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1164\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[0;32m   1165\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1166\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\J_C\\AppData\\Local\\Programs\\Python\\Python310\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[0;32m    181\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\Users\\J_C\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[0;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_qat_acc = 0\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "val_acc = []\n",
    "val_loss =[]\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    if epoch > 0 and epoch == qat_policy['start_epoch']:\n",
    "        print('QAT is starting!')\n",
    "        # Fuse the BN parameters into conv layers before Quantization Aware Training (QAT)\n",
    "        ai8x.fuse_bn_layers(model)\n",
    "        # Switch model from unquantized to quantized for QAT\n",
    "        ai8x.initiate_qat(model, qat_policy)\n",
    "        # Model is re-transferred to GPU in case parameters were added\n",
    "        model.to(device)\n",
    "    \n",
    "    ############ TRAIN SECTION ############\n",
    "    running_loss = []\n",
    "    acc = 0.\n",
    "    acc_weight = 0\n",
    "\n",
    "    train_start = time.time()\n",
    "    model.train()\n",
    "    for idx, (inputs, target) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model_out = model(inputs)\n",
    "        target_out = torch.argmax(model_out, dim=1)\n",
    "        \n",
    "        tp = torch.sum(target_out == target)\n",
    "        acc_batch = (tp / target_out.numel()).detach().item()\n",
    "        acc += target_out.shape[0] * acc_batch\n",
    "        acc_weight += target_out.shape[0]\n",
    "        \n",
    "        loss = criterion(model_out, target)\n",
    "\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch)        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "    \n",
    "    total_acc = 100 * (acc / acc_weight)\n",
    "    mean_loss = np.mean(running_loss)\n",
    "\n",
    "    # TRAIN ACCURACY / TRAIN LOSS\n",
    "    train_acc.append(total_acc)\n",
    "    train_loss.append(mean_loss)\n",
    "\n",
    "    train_end = time.time()\n",
    "    print(\"Epoch: {}/{}\\t LR: {}\\t \\t Dur: {:.2f} sec.\".format(\n",
    "        epoch+1, num_epochs, ms_lr_scheduler.get_lr(), (train_end-train_start)))\n",
    "    \n",
    "    \n",
    "    ############ VALIDATION SECTION ############\n",
    "\n",
    "    model.eval()\n",
    "    acc = 0.\n",
    "    acc_weight = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    running_v_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, target in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            model_out = model(inputs)\n",
    "            target_out = torch.argmax(model_out, dim=1)\n",
    "            \n",
    "            y_pred.extend(target_out.cpu().numpy())\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            \n",
    "            tp = torch.sum(target_out == target)\n",
    "            acc_batch = (tp / target_out.numel()).detach().item()\n",
    "            acc += target_out.shape[0] * acc_batch\n",
    "            acc_weight += target_out.shape[0]\n",
    "\n",
    "            v_loss = criterion(model_out, target)\n",
    "            running_v_loss.append(loss.cpu().detach().numpy())\n",
    "    \n",
    "        mean_loss = np.mean(running_v_loss)\n",
    "        total_acc = 100 * (acc / acc_weight)\n",
    "\n",
    "        if epoch == qat_policy['start_epoch']: best_acc = 0\n",
    "        \n",
    "        # VALIDATION ACCURACY / VALIDATION LOSS\n",
    "        val_acc.append(total_acc)\n",
    "        val_loss.append(mean_loss)\n",
    "\n",
    "        if total_acc > best_acc:\n",
    "            best_acc = total_acc\n",
    "            checkpoint_extras = {'current_top1': best_acc,\n",
    "                                 'best_top1': best_acc,\n",
    "                                 'best_epoch': epoch}\n",
    "            \n",
    "            model_prefix = f'{model_name}' if epoch < qat_policy['start_epoch'] else (f'qat_{model_name}')\n",
    "            apputils.save_checkpoint(epoch, model_name, model, optimizer=optimizer,\n",
    "                                     scheduler=None, extras=checkpoint_extras,\n",
    "                                     is_best=True, name=model_prefix,\n",
    "                                     dir=checkpoint_dir)\n",
    "            print(f'Best model saved with accuracy: {best_acc:.2f}%')\n",
    "            \n",
    "        #print('\\t\\t Test Acc: {:.2f}'.format(total_acc))\n",
    "        print(\"\\tConfusion:\")\n",
    "        plot_confusion(y_true, y_pred, classes)\n",
    "\n",
    "    ms_lr_scheduler.step()\n",
    "\n",
    "    print('----------------------')\n",
    "    print('Train Acc : ', train_acc[-1])\n",
    "    print('Train Loss : ', train_loss[-1])\n",
    "    print('Val Acc : ', val_acc[-1])\n",
    "    print('Val Loss : ', val_loss[-1])\n",
    "    print('----------------------')\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        plt.figure(epoch,figsize=(20,10))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(train_acc, color = 'red')\n",
    "        plt.plot(val_acc, color ='blue')\n",
    "        plt.legend(['Train','Validation'])\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Value')\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(train_loss,color='red')\n",
    "        plt.plot(val_loss,color='blue')\n",
    "        plt.legend(['Train','Validation'])\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Value')\n",
    "\n",
    "        plt.savefig(plot_dir+str(epoch)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''plt.figure(4,figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_acc, color = 'red')\n",
    "plt.plot(val_acc, color ='blue')\n",
    "plt.legend(['Train','Validation'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_loss,color='red')\n",
    "plt.plot(val_loss,color='blue')\n",
    "plt.legend(['Train','Validation'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.savefig(model_name+'_check.png')\n",
    "#plt.show()'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE ON HORSE COUGH AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes):\n",
    "    cf_matrix = confusion_matrix(y_true = y_true, y_pred = y_pred, labels =list(range(len(classes))))\n",
    "    print(cf_matrix)\n",
    "\n",
    "best_acc = 0\n",
    "best_qat_acc = 0\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "val_acc = []\n",
    "val_loss =[]\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    if epoch > 0 and epoch == qat_policy['start_epoch']:\n",
    "        print('QAT is starting!')\n",
    "        # Fuse the BN parameters into conv layers before Quantization Aware Training (QAT)\n",
    "        ai8x.fuse_bn_layers(model)\n",
    "        # Switch model from unquantized to quantized for QAT\n",
    "        ai8x.initiate_qat(model, qat_policy)\n",
    "        # Model is re-transferred to GPU in case parameters were added\n",
    "        model.to(device)\n",
    "    \n",
    "    ############ VALIDATION SECTION ############\n",
    "\n",
    "    model.eval()\n",
    "    acc = 0.\n",
    "    acc_weight = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    running_v_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, target in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            model_out = model(inputs)\n",
    "            target_out = torch.argmax(model_out, dim=1)\n",
    "            \n",
    "            y_pred.extend(target_out.cpu().numpy())\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            \n",
    "            tp = torch.sum(target_out == target)\n",
    "            acc_batch = (tp / target_out.numel()).detach().item()\n",
    "            acc += target_out.shape[0] * acc_batch\n",
    "            acc_weight += target_out.shape[0]\n",
    "\n",
    "            v_loss = criterion(model_out, target)\n",
    "            running_v_loss.append(loss.cpu().detach().numpy())\n",
    "    \n",
    "        mean_loss = np.mean(running_v_loss)\n",
    "        total_acc = 100 * (acc / acc_weight)\n",
    "\n",
    "        if epoch == qat_policy['start_epoch']: best_acc = 0\n",
    "        \n",
    "        # VALIDATION ACCURACY / VALIDATION LOSS\n",
    "        val_acc.append(total_acc)\n",
    "        val_loss.append(mean_loss)\n",
    "\n",
    "        if total_acc > best_acc:\n",
    "            best_acc = total_acc\n",
    "            checkpoint_extras = {'current_top1': best_acc,\n",
    "                                 'best_top1': best_acc,\n",
    "                                 'best_epoch': epoch}\n",
    "            model_name = 'ai85net_kws_equine_binary_scraped'\n",
    "            model_prefix = f'{model_name}' if epoch < qat_policy['start_epoch'] else (f'qat_{model_name}')\n",
    "            apputils.save_checkpoint(epoch, model_name, model, optimizer=optimizer,\n",
    "                                     scheduler=None, extras=checkpoint_extras,\n",
    "                                     is_best=True, name=model_prefix,\n",
    "                                     dir='.')\n",
    "            print(f'Best model saved with accuracy: {best_acc:.2f}%')\n",
    "            \n",
    "        #print('\\t\\t Test Acc: {:.2f}'.format(total_acc))\n",
    "        print(\"\\tConfusion:\")\n",
    "        plot_confusion(y_true, y_pred, classes)\n",
    "\n",
    "    ms_lr_scheduler.step()\n",
    "\n",
    "    print('----------------------')\n",
    "    print('Train Acc : ', train_acc[-1])\n",
    "    print('Train Loss : ', train_loss[-1])\n",
    "    print('Val Acc : ', val_acc[-1])\n",
    "    print('Val Loss : ', val_loss[-1])\n",
    "    print('----------------------')\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_acc, color = 'red')\n",
    "plt.plot(val_acc, color ='blue')\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_loss,color='red')\n",
    "plt.plot(val_loss,color='blue')\n",
    "plt.savefig(model_name+'.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec35e5d0984cc59eb0cd6a0be286daf2d556405b3fb6375200d13e76db69dcf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
