{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "## Load Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "from distiller import apputils\n",
    "import ai8x\n",
    "\n",
    "kws20 = importlib.import_module(\"datasets.kws20-horsecough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward: 0.0120192\n",
      "bed: 0.0099305\n",
      "bird: 0.0096899\n",
      "cat: 0.0098474\n",
      "dog: 0.0093985\n",
      "down: 0.0051059\n",
      "eight: 0.0052812\n",
      "five: 0.0049358\n",
      "follow: 0.0126662\n",
      "forward: 0.0128452\n",
      "four: 0.0053648\n",
      "go: 0.0051546\n",
      "happy: 0.0097371\n",
      "horse_cough: 0.3773585\n",
      "horse_neigh: 0.7407407\n",
      "house: 0.0094652\n",
      "human_cough: 1.0\n",
      "learn: 0.0126984\n",
      "left: 0.0052618\n",
      "marvin: 0.0095238\n",
      "nine: 0.0050839\n",
      "no: 0.0050749\n",
      "off: 0.0053405\n",
      "on: 0.0052016\n",
      "one: 0.0051414\n",
      "right: 0.0052938\n",
      "seven: 0.0050025\n",
      "sheila: 0.0098912\n",
      "six: 0.0051813\n",
      "stop: 0.0051653\n",
      "three: 0.0053662\n",
      "tree: 0.0113701\n",
      "two: 0.0051546\n",
      "up: 0.005372\n",
      "visual: 0.0125628\n",
      "wow: 0.0094206\n",
      "yes: 0.0049456\n",
      "zero: 0.0049358\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "raw_data_path = Path(\"../data/KWS/raw/\")\n",
    "class_file_count = {}\n",
    "\n",
    "class_dirs = [d for d in raw_data_path.iterdir() if d.is_dir() and d.stem != \"_background_noise_\"]\n",
    "\n",
    "for d in class_dirs:\n",
    "    class_file_count[d] = len(list(d.iterdir()))\n",
    "\n",
    "min_file_count = float(min(class_file_count.values()))\n",
    "\n",
    "for d in class_dirs:\n",
    "    class_file_count[d] = min_file_count / class_file_count[d]\n",
    "    print(f\"{d.stem}: {round(class_file_count[d], 7)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No key `noise_var` in input augmentation dictionary!  Using defaults: [Min: 0., Max: 1.]\n",
      "No key `shift` in input augmentation dictionary! Using defaults: [Min:-0.1, Max: 0.1]\n",
      "No key `strech` in input augmentation dictionary! Using defaults: [Min: 0.8, Max: 1.3]\n",
      "Using downloaded and verified file: ../data\\KWS\\raw\\speech_commands_v0.02.tar.gz\n",
      "Extracting ../data\\KWS\\raw\\speech_commands_v0.02.tar.gz to ../data\\KWS\\raw\n",
      "Generating dataset from raw data samples for the first time. \n",
      "This process will take significant time (~60 minutes)...\n",
      "data_len: 16384\n",
      "------------- Label Size ---------------\n",
      "backward:  \t1664\n",
      "bed     :  \t2014\n",
      "bird    :  \t2064\n",
      "cat     :  \t2031\n",
      "dog     :  \t2128\n",
      "down    :  \t3917\n",
      "eight   :  \t3787\n",
      "five    :  \t4052\n",
      "follow  :  \t1579\n",
      "forward :  \t1557\n",
      "four    :  \t3728\n",
      "go      :  \t3880\n",
      "happy   :  \t2054\n",
      "horse_cough:  \t53\n",
      "horse_neigh:  \t27\n",
      "house   :  \t2113\n",
      "human_cough:  \t20\n",
      "learn   :  \t1575\n",
      "left    :  \t3801\n",
      "marvin  :  \t2100\n",
      "nine    :  \t3934\n",
      "no      :  \t3941\n",
      "off     :  \t3745\n",
      "on      :  \t3845\n",
      "one     :  \t3890\n",
      "right   :  \t3778\n",
      "seven   :  \t3998\n",
      "sheila  :  \t2022\n",
      "six     :  \t3860\n",
      "stop    :  \t3872\n",
      "three   :  \t3727\n",
      "tree    :  \t1759\n",
      "two     :  \t3880\n",
      "up      :  \t3723\n",
      "visual  :  \t1592\n",
      "wow     :  \t2123\n",
      "yes     :  \t4044\n",
      "zero    :  \t4052\n",
      "------------------------------------------\n",
      "Processing the label: backward. 1 of 38\n",
      "\t1 of 1664\n",
      "\t1001 of 1664\n",
      "Finished in 43.626 seconds.\n",
      "(4992, 128, 128)\n",
      "Data concatenation finished in 0.024 seconds.\n",
      "Processing the label: bed. 2 of 38\n",
      "\t1 of 2014\n",
      "\t1001 of 2014\n",
      "\t2001 of 2014\n",
      "Finished in 49.862 seconds.\n",
      "(6042, 128, 128)\n",
      "Data concatenation finished in 0.057 seconds.\n",
      "Processing the label: bird. 3 of 38\n",
      "\t1 of 2064\n",
      "\t1001 of 2064\n",
      "\t2001 of 2064\n",
      "Finished in 51.589 seconds.\n",
      "(6192, 128, 128)\n",
      "Data concatenation finished in 0.101 seconds.\n",
      "Processing the label: cat. 4 of 38\n",
      "\t1 of 2031\n",
      "\t1001 of 2031\n",
      "\t2001 of 2031\n",
      "Finished in 49.851 seconds.\n",
      "(6093, 128, 128)\n",
      "Data concatenation finished in 0.145 seconds.\n",
      "Processing the label: dog. 5 of 38\n",
      "\t1 of 2128\n",
      "\t1001 of 2128\n",
      "\t2001 of 2128\n",
      "Finished in 52.115 seconds.\n",
      "(6384, 128, 128)\n",
      "Data concatenation finished in 0.151 seconds.\n",
      "Processing the label: down. 6 of 38\n",
      "\t1 of 3917\n",
      "\t1001 of 3917\n",
      "\t2001 of 3917\n",
      "\t3001 of 3917\n",
      "Finished in 96.500 seconds.\n",
      "(11751, 128, 128)\n",
      "Data concatenation finished in 0.196 seconds.\n",
      "Processing the label: eight. 7 of 38\n",
      "\t1 of 3787\n",
      "\t1001 of 3787\n",
      "\t2001 of 3787\n",
      "\t3001 of 3787\n",
      "Finished in 95.519 seconds.\n",
      "(11361, 128, 128)\n",
      "Data concatenation finished in 0.253 seconds.\n",
      "Processing the label: five. 8 of 38\n",
      "\t1 of 4052\n",
      "\t1001 of 4052\n",
      "\t2001 of 4052\n",
      "\t3001 of 4052\n",
      "\t4001 of 4052\n",
      "Finished in 99.349 seconds.\n",
      "(12156, 128, 128)\n",
      "Data concatenation finished in 0.295 seconds.\n",
      "Processing the label: follow. 9 of 38\n",
      "\t1 of 1579\n",
      "\t1001 of 1579\n",
      "Finished in 38.694 seconds.\n",
      "(4737, 128, 128)\n",
      "Data concatenation finished in 0.358 seconds.\n",
      "Processing the label: forward. 10 of 38\n",
      "\t1 of 1557\n",
      "\t1001 of 1557\n",
      "Finished in 38.530 seconds.\n",
      "(4671, 128, 128)\n",
      "Data concatenation finished in 0.362 seconds.\n",
      "Processing the label: four. 11 of 38\n",
      "\t1 of 3728\n",
      "\t1001 of 3728\n",
      "\t2001 of 3728\n",
      "\t3001 of 3728\n",
      "Finished in 91.627 seconds.\n",
      "(11184, 128, 128)\n",
      "Data concatenation finished in 0.465 seconds.\n",
      "Processing the label: go. 12 of 38\n",
      "\t1 of 3880\n",
      "\t1001 of 3880\n",
      "\t2001 of 3880\n",
      "\t3001 of 3880\n",
      "Finished in 97.670 seconds.\n",
      "(11640, 128, 128)\n",
      "Data concatenation finished in 0.483 seconds.\n",
      "Processing the label: happy. 13 of 38\n",
      "\t1 of 2054\n",
      "\t1001 of 2054\n",
      "\t2001 of 2054\n",
      "Finished in 50.153 seconds.\n",
      "(6162, 128, 128)\n",
      "Data concatenation finished in 0.460 seconds.\n",
      "Processing the label: horse_cough. 14 of 38\n",
      "\t1 of 53\n",
      "Finished in 1.494 seconds.\n",
      "(159, 128, 128)\n",
      "Data concatenation finished in 0.447 seconds.\n",
      "Processing the label: horse_neigh. 15 of 38\n",
      "\t1 of 27\n",
      "Finished in 0.779 seconds.\n",
      "(81, 128, 128)\n",
      "Data concatenation finished in 0.492 seconds.\n",
      "Processing the label: house. 16 of 38\n",
      "\t1 of 2113\n",
      "\t1001 of 2113\n",
      "\t2001 of 2113\n",
      "Finished in 53.162 seconds.\n",
      "(6339, 128, 128)\n",
      "Data concatenation finished in 0.489 seconds.\n",
      "Processing the label: human_cough. 17 of 38\n",
      "\t1 of 20\n",
      "Finished in 0.575 seconds.\n",
      "(60, 128, 128)\n",
      "Data concatenation finished in 0.474 seconds.\n",
      "Processing the label: learn. 18 of 38\n",
      "\t1 of 1575\n",
      "\t1001 of 1575\n",
      "Finished in 38.565 seconds.\n",
      "(4725, 128, 128)\n",
      "Data concatenation finished in 0.496 seconds.\n",
      "Processing the label: left. 19 of 38\n",
      "\t1 of 3801\n",
      "\t1001 of 3801\n",
      "\t2001 of 3801\n",
      "\t3001 of 3801\n",
      "Finished in 96.689 seconds.\n",
      "(11403, 128, 128)\n",
      "Data concatenation finished in 0.677 seconds.\n",
      "Processing the label: marvin. 20 of 38\n",
      "\t1 of 2100\n",
      "\t1001 of 2100\n",
      "\t2001 of 2100\n",
      "Finished in 53.145 seconds.\n",
      "(6300, 128, 128)\n",
      "Data concatenation finished in 0.674 seconds.\n",
      "Processing the label: nine. 21 of 38\n",
      "\t1 of 3934\n",
      "\t1001 of 3934\n",
      "\t2001 of 3934\n",
      "\t3001 of 3934\n",
      "Finished in 98.988 seconds.\n",
      "(11802, 128, 128)\n",
      "Data concatenation finished in 0.679 seconds.\n",
      "Processing the label: no. 22 of 38\n",
      "\t1 of 3941\n",
      "\t1001 of 3941\n",
      "\t2001 of 3941\n",
      "\t3001 of 3941\n",
      "Finished in 99.176 seconds.\n",
      "(11823, 128, 128)\n",
      "Data concatenation finished in 0.770 seconds.\n",
      "Processing the label: off. 23 of 38\n",
      "\t1 of 3745\n",
      "\t1001 of 3745\n",
      "\t2001 of 3745\n",
      "\t3001 of 3745\n",
      "Finished in 94.862 seconds.\n",
      "(11235, 128, 128)\n",
      "Data concatenation finished in 0.818 seconds.\n",
      "Processing the label: on. 24 of 38\n",
      "\t1 of 3845\n",
      "\t1001 of 3845\n",
      "\t2001 of 3845\n",
      "\t3001 of 3845\n",
      "Finished in 97.696 seconds.\n",
      "(11535, 128, 128)\n",
      "Data concatenation finished in 0.975 seconds.\n",
      "Processing the label: one. 25 of 38\n",
      "\t1 of 3890\n",
      "\t1001 of 3890\n",
      "\t2001 of 3890\n",
      "\t3001 of 3890\n",
      "Finished in 97.622 seconds.\n",
      "(11670, 128, 128)\n",
      "Data concatenation finished in 1.852 seconds.\n",
      "Processing the label: right. 26 of 38\n",
      "\t1 of 3778\n",
      "\t1001 of 3778\n",
      "\t2001 of 3778\n",
      "\t3001 of 3778\n",
      "Finished in 95.508 seconds.\n",
      "(11334, 128, 128)\n",
      "Data concatenation finished in 1.000 seconds.\n",
      "Processing the label: seven. 27 of 38\n",
      "\t1 of 3998\n",
      "\t1001 of 3998\n",
      "\t2001 of 3998\n",
      "\t3001 of 3998\n",
      "Finished in 100.948 seconds.\n",
      "(11994, 128, 128)\n",
      "Data concatenation finished in 1.004 seconds.\n",
      "Processing the label: sheila. 28 of 38\n",
      "\t1 of 2022\n",
      "\t1001 of 2022\n",
      "\t2001 of 2022\n",
      "Finished in 51.284 seconds.\n",
      "(6066, 128, 128)\n",
      "Data concatenation finished in 3.450 seconds.\n",
      "Processing the label: six. 29 of 38\n",
      "\t1 of 3860\n",
      "\t1001 of 3860\n",
      "\t2001 of 3860\n",
      "\t3001 of 3860\n",
      "Finished in 98.275 seconds.\n",
      "(11580, 128, 128)\n",
      "Data concatenation finished in 3.041 seconds.\n",
      "Processing the label: stop. 30 of 38\n",
      "\t1 of 3872\n",
      "\t1001 of 3872\n",
      "\t2001 of 3872\n",
      "\t3001 of 3872\n",
      "Finished in 97.368 seconds.\n",
      "(11616, 128, 128)\n",
      "Data concatenation finished in 2.538 seconds.\n",
      "Processing the label: three. 31 of 38\n",
      "\t1 of 3727\n",
      "\t1001 of 3727\n",
      "\t2001 of 3727\n",
      "\t3001 of 3727\n",
      "Finished in 93.660 seconds.\n",
      "(11181, 128, 128)\n",
      "Data concatenation finished in 2.475 seconds.\n",
      "Processing the label: tree. 32 of 38\n",
      "\t1 of 1759\n",
      "\t1001 of 1759\n",
      "Finished in 45.009 seconds.\n",
      "(5277, 128, 128)\n",
      "Data concatenation finished in 1.409 seconds.\n",
      "Processing the label: two. 33 of 38\n",
      "\t1 of 3880\n",
      "\t1001 of 3880\n",
      "\t2001 of 3880\n",
      "\t3001 of 3880\n",
      "Finished in 101.986 seconds.\n",
      "(11640, 128, 128)\n",
      "Data concatenation finished in 3.586 seconds.\n",
      "Processing the label: up. 34 of 38\n",
      "\t1 of 3723\n",
      "\t1001 of 3723\n",
      "\t2001 of 3723\n",
      "\t3001 of 3723\n",
      "Finished in 94.178 seconds.\n",
      "(11169, 128, 128)\n",
      "Data concatenation finished in 2.540 seconds.\n",
      "Processing the label: visual. 35 of 38\n",
      "\t1 of 1592\n",
      "\t1001 of 1592\n",
      "Finished in 40.081 seconds.\n",
      "(4776, 128, 128)\n",
      "Data concatenation finished in 2.320 seconds.\n",
      "Processing the label: wow. 36 of 38\n",
      "\t1 of 2123\n",
      "\t1001 of 2123\n",
      "\t2001 of 2123\n",
      "Finished in 53.252 seconds.\n",
      "(6369, 128, 128)\n",
      "Data concatenation finished in 1.764 seconds.\n",
      "Processing the label: yes. 37 of 38\n",
      "\t1 of 4044\n",
      "\t1001 of 4044\n",
      "\t2001 of 4044\n",
      "\t3001 of 4044\n",
      "\t4001 of 4044\n",
      "Finished in 101.733 seconds.\n",
      "(12132, 128, 128)\n",
      "Data concatenation finished in 1.474 seconds.\n",
      "Processing the label: zero. 38 of 38\n",
      "\t1 of 4052\n",
      "\t1001 of 4052\n",
      "\t2001 of 4052\n",
      "\t3001 of 4052\n",
      "\t4001 of 4052\n",
      "Finished in 102.121 seconds.\n",
      "(12156, 128, 128)\n",
      "Data concatenation finished in 2.715 seconds.\n",
      "Dataset created.\n",
      "Training+Validation: 3671,  Test: 381\n",
      "\n",
      "Processing train...\n",
      "Class horse_cough (# 13): 147 elements\n",
      "Class horse_neigh not found in data\n",
      "\n",
      "Processing test...\n",
      "Class horse_cough (# 13): 12 elements\n",
      "Class horse_neigh not found in data\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'truncate_testset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_batch_size \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n\u001b[1;32m----> 2\u001b[0m train_loader, val_loader, test_loader, _ \u001b[39m=\u001b[39m apputils\u001b[39m.\u001b[39;49mget_data_loaders(\n\u001b[0;32m      3\u001b[0m     kws20\u001b[39m.\u001b[39;49mKWS_HORSE_TF_get_datasets, (\u001b[39m\"\u001b[39;49m\u001b[39m../data\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m), train_batch_size, \u001b[39m1\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataset sizes:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mtraining=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_loader\u001b[39m.\u001b[39msampler)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mvalidation=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(val_loader\u001b[39m.\u001b[39msampler)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mtest=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(test_loader\u001b[39m.\u001b[39msampler)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\mtinaco\\dev\\max78k\\ai8x-training\\distiller\\distiller\\apputils\\data_loaders.py:292\u001b[0m, in \u001b[0;36mget_data_loaders\u001b[1;34m(datasets_fn, data_dir, batch_size, num_workers, validation_split, deterministic, effective_train_size, effective_valid_size, effective_test_size, fixed_subset, sequential, test_only, collate_fn, cpu)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_data_loaders\u001b[39m(datasets_fn, data_dir, batch_size, num_workers, validation_split\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, deterministic\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    290\u001b[0m                      effective_train_size\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m, effective_valid_size\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m, effective_test_size\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m, fixed_subset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    291\u001b[0m                      sequential\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, test_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, collate_fn\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cpu\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 292\u001b[0m     train_dataset, test_dataset \u001b[39m=\u001b[39m datasets_fn(data_dir, load_train\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m test_only, load_test\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    294\u001b[0m     worker_init_fn \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    295\u001b[0m     \u001b[39mif\u001b[39;00m deterministic:\n",
      "File \u001b[1;32mc:\\Users\\MTinaco\\Dev\\max78k\\ai8x-training\\notebooks\\..\\datasets\\kws20-horsecough.py:612\u001b[0m, in \u001b[0;36mKWS_HORSE_TF_get_datasets\u001b[1;34m(data, load_train, load_test)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mKWS_HORSE_TF_get_datasets\u001b[39m(data, load_train\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, load_test\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 612\u001b[0m     \u001b[39mreturn\u001b[39;00m KWS_get_datasets(data, load_train, load_test, num_classes\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\MTinaco\\Dev\\max78k\\ai8x-training\\notebooks\\..\\datasets\\kws20-horsecough.py:566\u001b[0m, in \u001b[0;36mKWS_get_datasets\u001b[1;34m(data, load_train, load_test, num_classes)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[39mif\u001b[39;00m load_test:\n\u001b[0;32m    561\u001b[0m     test_dataset \u001b[39m=\u001b[39m KWS(root\u001b[39m=\u001b[39mdata_dir, classes\u001b[39m=\u001b[39mclasses, d_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    562\u001b[0m                        transform\u001b[39m=\u001b[39mtransform, t_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mkeyword\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    563\u001b[0m                        quantization_scheme\u001b[39m=\u001b[39mquantization_scheme,\n\u001b[0;32m    564\u001b[0m                        augmentation\u001b[39m=\u001b[39maugmentation, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 566\u001b[0m     \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39;49mtruncate_testset:\n\u001b[0;32m    567\u001b[0m         test_dataset\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m test_dataset\u001b[39m.\u001b[39mdata[:\u001b[39m1\u001b[39m]\n\u001b[0;32m    568\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'truncate_testset'"
     ]
    }
   ],
   "source": [
    "train_batch_size = 128\n",
    "train_loader, val_loader, test_loader, _ = apputils.get_data_loaders(\n",
    "    kws20.KWS_HORSE_TF_get_datasets, (\"../data\", False), train_batch_size, 1, validation_split=0)\n",
    "print(f\"Dataset sizes:\\n\\ttraining={len(train_loader.sampler)}\\n\\tvalidation={len(val_loader.sampler)}\\n\\ttest={len(test_loader.sampler)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"horse_cough\", \"horse_neigh\", \"human_cough\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai8x.set_device(device=85, simulate=False, round_avg=False)\n",
    "\n",
    "mod = importlib.import_module(\"models.ai85net-kws20-kabayo\")\n",
    "\n",
    "model = mod.AI85KWS20Netv3(num_classes=21, num_channels=128, dimensions=(128, 1), bias=False)\n",
    "print(f'Number of Model Params: {count_params(model)}')\n",
    "\n",
    "# model, compression_scheduler, optimizer, start_epoch = apputils.load_checkpoint(\n",
    "#             model, \"logs/2023.01.10-013008/qat_best.pth.tar\", model_device='cuda')\n",
    "\n",
    "model, compression_scheduler, optimizer, start_epoch = apputils.load_checkpoint(\n",
    "            model, \"logs/2023.01.23-053753/qat_best.pth.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace FC layer and freeze the rest of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layer(layer):\n",
    "    for p in layer.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_layer(model.voice_conv1)\n",
    "freeze_layer(model.voice_conv2)\n",
    "freeze_layer(model.voice_conv3)\n",
    "freeze_layer(model.voice_conv4)\n",
    "freeze_layer(model.kws_conv1)\n",
    "# freeze_layer(model.kws_conv2)\n",
    "# freeze_layer(model.kws_conv3)\n",
    "# freeze_layer(model.kws_conv4)\n",
    "model.fc = ai8x.Linear(256, 8, bias=False, wide=True)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "epoch = 0\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "ms_lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 80], gamma=0.5)\n",
    "criterion = torch.nn.CrossEntropyLoss(\n",
    "    torch.Tensor((1, 1, 0.01, 1, 1, 0.01, 0.01, 0.01))\n",
    ")\n",
    "criterion.to(device)\n",
    "\n",
    "qat_policy = {\n",
    "    'start_epoch': 20,\n",
    "    'weight_bits': 8\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes):\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred, list(range(len(classes))))\n",
    "    print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "best_qat_acc = 0\n",
    "for epoch in range(0, num_epochs):\n",
    "    if epoch > 0 and epoch == qat_policy['start_epoch']:\n",
    "        print('QAT is starting!')\n",
    "        # Fuse the BN parameters into conv layers before Quantization Aware Training (QAT)\n",
    "        ai8x.fuse_bn_layers(model)\n",
    "\n",
    "        # Switch model from unquantized to quantized for QAT\n",
    "        ai8x.initiate_qat(model, qat_policy)\n",
    "\n",
    "        # Model is re-transferred to GPU in case parameters were added\n",
    "        model.to(device)\n",
    "    running_loss = []\n",
    "    train_start = time.time()\n",
    "    model.train()\n",
    "    for idx, (inputs, target) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model_out = model(inputs)\n",
    "        \n",
    "        loss = criterion(model_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "    mean_loss = np.mean(running_loss)\n",
    "    train_end = time.time()\n",
    "    print(\"Epoch: {}/{}\\t LR: {}\\t Train Loss: {:.4f}\\t Dur: {:.2f} sec.\".format(\n",
    "        epoch+1, num_epochs, ms_lr_scheduler.get_lr(), mean_loss, (train_end-train_start)))\n",
    "    \n",
    "    model.eval()\n",
    "    acc = 0.\n",
    "    acc_weight = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            model_out = model(inputs)\n",
    "            target_out = torch.argmax(model_out, dim=1)\n",
    "            \n",
    "            y_pred.extend(target_out.cpu().numpy())\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            \n",
    "            tp = torch.sum(target_out == target)\n",
    "            acc_batch = (tp / target_out.numel()).detach().item()\n",
    "            acc += target_out.shape[0] * acc_batch\n",
    "            acc_weight += target_out.shape[0]\n",
    "            \n",
    "        total_acc = 100 * (acc / acc_weight)\n",
    "        if epoch == qat_policy['start_epoch']: best_acc = 0\n",
    "        if total_acc > best_acc:\n",
    "            best_acc = total_acc\n",
    "            checkpoint_extras = {'current_top1': best_acc,\n",
    "                                 'best_top1': best_acc,\n",
    "                                 'best_epoch': epoch}\n",
    "            model_name = 'ai85net_kws_dash'\n",
    "            model_prefix = f'{model_name}' if epoch < qat_policy['start_epoch'] else (f'qat_{model_name}')\n",
    "            apputils.save_checkpoint(epoch, model_name, model, optimizer=optimizer,\n",
    "                                     scheduler=None, extras=checkpoint_extras,\n",
    "                                     is_best=True, name=model_prefix,\n",
    "                                     dir='.')\n",
    "            print(f'Best model saved with accuracy: {best_acc:.2f}%')\n",
    "            \n",
    "        print('\\t\\t Test Acc: {:.2f}'.format(total_acc))\n",
    "        print(\"\\t\\tConfusion:\")\n",
    "        plot_confusion(y_true, y_pred, classes)\n",
    "    ms_lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33335497366de2c65b60bad2619bdb398d3af58569807f2e4d877bfe78d46b65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
